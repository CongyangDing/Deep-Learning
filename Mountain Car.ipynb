{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BipedalWalker-v3')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tile Coding Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_interval(t,a,b,n):\n",
    "        if True in [((t <= a + (i+1)*(b-a)/n) and (t > a + i*(b-a)/n)) for  i in range(n)]:\n",
    "            return  [((t <= a + (i+1)*(b-a)/n) and (t > a + i*(b-a)/n)) for  i in range(n)].index(True)\n",
    "        else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X(s):\n",
    "    enc = []\n",
    "    for k in range(8):\n",
    "        i = sub_interval(s[0],-1.2-k*0.22/8,0.72-k*0.22/8,8)\n",
    "        j = sub_interval(s[1],-0.07 - k*0.01/8,0.08-k*0.01/8,8)\n",
    "        u = np.zeros(64)\n",
    "        if i != None and j!= None: \n",
    "            u[8*i + j] = 1\n",
    "        enc.append(u)\n",
    "    return np.hstack(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X([-0.9,0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([[0,0,1],[0,1,0],[1,0,0]])\n",
    "def X(s,a):\n",
    "    enc = []\n",
    "    for k in range(8):\n",
    "        i = sub_interval(s[0],-1.2-k*0.22/8,0.72-k*0.22/8,8)\n",
    "        j = sub_interval(s[1],-0.07 - k*0.01/8,0.08-k*0.01/8,8)\n",
    "        u = np.zeros(64)\n",
    "        if i != None and j!= None: \n",
    "            u[8*i + j] = 1\n",
    "        enc.append(u)\n",
    "    return np.append(np.hstack(enc),actions[a])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-93b811574c89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mupdate_counter\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0ms_prime\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSPEED_KNEE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSPEED_HIP\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m             self.joints[0].maxMotorTorque = float(\n\u001b[0;32m    416\u001b[0m                 \u001b[0mMOTORS_TORQUE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "a = 1\n",
    "update_frequency = 500\n",
    "update_counter = 0\n",
    "w = np.zeros(8*8*8 + 3)\n",
    "w_fixed = np.zeros(8*8*8 + 3)\n",
    "eps = 1.0\n",
    "decay = 0.99\n",
    "alpha = 1e-3\n",
    "done = False\n",
    "gamma = 0.995\n",
    "r = 0\n",
    "params = []\n",
    "rewards = []\n",
    "for k in range(2000):\n",
    "    \n",
    "    s = env.reset()\n",
    "    a = 2\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        update_counter+=1\n",
    "        s_prime,r,done,_ = env.step(a)\n",
    "        \n",
    "        t = np.random.random()\n",
    "        if t < eps:\n",
    "            a_prime = np.random.choice([0,1,2])\n",
    "        else:\n",
    "            a_prime = np.argmax([X(s_prime,a)@w for a in [0,1,2]])\n",
    "    \n",
    "        if update_counter > update_frequency:\n",
    "            w_fixed = w\n",
    "            update_counter = 0\n",
    "            print('updated target')\n",
    "        \n",
    "        w = w + alpha * (r + gamma*X(s_prime,a_prime) @ w_fixed - X(s,a) @ w) * X(s,a)\n",
    "        s = s_prime\n",
    "        a = a_prime\n",
    "        eps = eps * decay\n",
    "        total_reward += r\n",
    "        \n",
    "        env.render()\n",
    "    params.append(w)\n",
    "    print(k , total_reward)\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-709eef2b7b97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rewards' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(rewards);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1645"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = params[1645]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-97.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[1645]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(s,a):\n",
    "    return X(s,a) @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e39cff0f6fa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSPEED_KNEE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSPEED_HIP\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m             self.joints[0].maxMotorTorque = float(\n\u001b[0;32m    416\u001b[0m                 \u001b[0mMOTORS_TORQUE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        a =np.argmax([Q(s,a) for a in [0,1,2]])\n",
    "        s, r,done,_ = env.step(a)\n",
    "        env.render()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a3c6ff44d30f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0ms_prime\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep_counter\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mupdate_frequency\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSPEED_KNEE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSPEED_HIP\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m             self.joints[0].maxMotorTorque = float(\n\u001b[0;32m    416\u001b[0m                 \u001b[0mMOTORS_TORQUE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "a = 1\n",
    "update_frequency = 500\n",
    "step_counter = 0\n",
    "w = np.zeros(8*8*8 + 3)\n",
    "w_target = np.zeros(8*8*8 + 3)\n",
    "eps = 1.0\n",
    "decay = 0.99\n",
    "alpha = 1e-3\n",
    "done = False\n",
    "gamma = 0.995\n",
    "r = -1\n",
    "params = []\n",
    "rewards = []\n",
    "for k in range(2000):\n",
    "    s = env.reset()\n",
    "    a = 1\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        step_counter+=1\n",
    "\n",
    "        t = np.random.random()\n",
    "        if t < eps:\n",
    "                a = np.random.choice([0,1,2])\n",
    "  \n",
    "        else:\n",
    "                a = np.argmax([X(s,b)@w for b in [0,1,2]])\n",
    "       \n",
    "        s_prime,r,done,_ = env.step(a)\n",
    "        \n",
    "        if step_counter > update_frequency:\n",
    "            w_target = w\n",
    "            step_counter = 0\n",
    "            print('updated target')\n",
    "\n",
    "        w = w + alpha * (r + gamma*np.max([X(s_prime,a) @ w_target for a in [0,1,2]]) - X(s,a) @ w) * X(s,a)\n",
    "        \n",
    "        \n",
    "        s = s_prime\n",
    "        eps = eps * decay\n",
    "        total_reward += r\n",
    "        if k > 500 and k % 50 == 0:\n",
    "            env.render()\n",
    "    params.append(w)\n",
    "    print(k , total_reward)\n",
    "    rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23c56313310>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -200.0\n",
      "1 -200.0\n",
      "2 -200.0\n",
      "3 -200.0\n",
      "4 -200.0\n",
      "5 -200.0\n",
      "6 -200.0\n",
      "7 -200.0\n",
      "8 -200.0\n",
      "9 -200.0\n",
      "10 -200.0\n",
      "11 -200.0\n",
      "12 -200.0\n",
      "13 -200.0\n",
      "14 -200.0\n",
      "15 -200.0\n",
      "16 -200.0\n",
      "17 -200.0\n",
      "18 -200.0\n",
      "19 -200.0\n",
      "20 -200.0\n",
      "21 -200.0\n",
      "22 -200.0\n",
      "23 -200.0\n",
      "24 -200.0\n",
      "25 -200.0\n",
      "26 -200.0\n",
      "27 -200.0\n",
      "28 -200.0\n",
      "29 -200.0\n",
      "30 -200.0\n",
      "31 -200.0\n",
      "32 -200.0\n",
      "33 -200.0\n",
      "34 -200.0\n",
      "35 -200.0\n",
      "36 -200.0\n",
      "37 -200.0\n",
      "38 -200.0\n",
      "39 -200.0\n",
      "40 -200.0\n",
      "41 -200.0\n",
      "42 -200.0\n",
      "43 -200.0\n",
      "44 -200.0\n",
      "45 -200.0\n",
      "46 -200.0\n",
      "47 -200.0\n",
      "48 -200.0\n",
      "49 -200.0\n",
      "50 -200.0\n",
      "51 -200.0\n",
      "52 -200.0\n",
      "53 -200.0\n",
      "54 -200.0\n",
      "55 -200.0\n",
      "56 -200.0\n",
      "57 -200.0\n",
      "58 -200.0\n",
      "59 -200.0\n",
      "60 -200.0\n",
      "61 -200.0\n",
      "62 -200.0\n",
      "63 -200.0\n",
      "64 -200.0\n",
      "65 -200.0\n",
      "66 -200.0\n",
      "67 -200.0\n",
      "68 -200.0\n",
      "69 -200.0\n",
      "70 -200.0\n",
      "71 -200.0\n",
      "72 -200.0\n",
      "73 -200.0\n",
      "74 -200.0\n",
      "75 -200.0\n",
      "76 -200.0\n",
      "77 -200.0\n",
      "78 -200.0\n",
      "79 -200.0\n",
      "80 -200.0\n",
      "81 -200.0\n",
      "82 -200.0\n",
      "83 -200.0\n",
      "84 -200.0\n",
      "85 -200.0\n",
      "86 -200.0\n",
      "87 -200.0\n",
      "88 -200.0\n",
      "89 -200.0\n",
      "90 -200.0\n",
      "91 -200.0\n",
      "92 -200.0\n",
      "93 -200.0\n",
      "94 -200.0\n",
      "95 -200.0\n",
      "96 -200.0\n",
      "97 -200.0\n",
      "98 -200.0\n",
      "99 -200.0\n",
      "100 -200.0\n",
      "101 -200.0\n",
      "102 -200.0\n",
      "103 -200.0\n",
      "104 -200.0\n",
      "105 -200.0\n",
      "106 -200.0\n",
      "107 -200.0\n",
      "108 -200.0\n",
      "109 -200.0\n",
      "110 -200.0\n",
      "111 -200.0\n",
      "112 -200.0\n",
      "113 -200.0\n",
      "114 -200.0\n",
      "115 -200.0\n",
      "116 -200.0\n",
      "117 -200.0\n",
      "118 -200.0\n",
      "119 -200.0\n",
      "120 -200.0\n",
      "121 -200.0\n",
      "122 -200.0\n",
      "123 -200.0\n",
      "124 -200.0\n",
      "125 -200.0\n",
      "126 -200.0\n",
      "127 -200.0\n",
      "128 -200.0\n",
      "129 -200.0\n",
      "130 -200.0\n",
      "131 -200.0\n",
      "132 -200.0\n",
      "133 -200.0\n",
      "134 -200.0\n",
      "135 -200.0\n",
      "136 -200.0\n",
      "137 -200.0\n",
      "138 -200.0\n",
      "139 -200.0\n",
      "140 -200.0\n",
      "141 -200.0\n",
      "142 -200.0\n",
      "143 -200.0\n",
      "144 -200.0\n",
      "145 -200.0\n",
      "146 -200.0\n",
      "147 -200.0\n",
      "148 -200.0\n",
      "149 -200.0\n",
      "150 -200.0\n",
      "151 -200.0\n",
      "152 -200.0\n",
      "153 -200.0\n",
      "154 -200.0\n",
      "155 -200.0\n",
      "156 -200.0\n",
      "157 -200.0\n",
      "158 -200.0\n",
      "159 -200.0\n",
      "160 -200.0\n",
      "161 -200.0\n",
      "162 -200.0\n",
      "163 -200.0\n",
      "164 -200.0\n",
      "165 -200.0\n",
      "166 -200.0\n",
      "167 -200.0\n",
      "168 -200.0\n",
      "169 -200.0\n",
      "170 -200.0\n",
      "171 -200.0\n",
      "172 -200.0\n",
      "173 -200.0\n",
      "174 -200.0\n",
      "175 -200.0\n",
      "176 -200.0\n",
      "177 -200.0\n",
      "178 -200.0\n",
      "179 -200.0\n",
      "180 -200.0\n",
      "181 -200.0\n",
      "182 -200.0\n",
      "183 -200.0\n",
      "184 -200.0\n",
      "185 -200.0\n",
      "186 -200.0\n",
      "187 -200.0\n",
      "188 -200.0\n",
      "189 -200.0\n",
      "190 -200.0\n",
      "191 -200.0\n",
      "192 -200.0\n",
      "193 -200.0\n",
      "194 -200.0\n",
      "195 -200.0\n",
      "196 -200.0\n",
      "197 -200.0\n",
      "198 -200.0\n",
      "199 -200.0\n",
      "200 -200.0\n",
      "201 -200.0\n",
      "202 -200.0\n",
      "203 -200.0\n",
      "204 -200.0\n",
      "205 -200.0\n",
      "206 -200.0\n",
      "207 -200.0\n",
      "208 -200.0\n",
      "209 -200.0\n",
      "210 -200.0\n",
      "211 -200.0\n",
      "212 -200.0\n",
      "213 -200.0\n",
      "214 -200.0\n",
      "215 -200.0\n",
      "216 -200.0\n",
      "217 -200.0\n",
      "218 -200.0\n",
      "219 -200.0\n",
      "220 -200.0\n",
      "221 -200.0\n",
      "222 -200.0\n",
      "223 -200.0\n",
      "224 -200.0\n",
      "225 -200.0\n",
      "226 -200.0\n",
      "227 -200.0\n",
      "228 -200.0\n",
      "229 -200.0\n",
      "230 -200.0\n",
      "231 -200.0\n",
      "232 -200.0\n",
      "233 -200.0\n",
      "234 -200.0\n",
      "235 -200.0\n",
      "236 -200.0\n",
      "237 -200.0\n",
      "238 -200.0\n",
      "239 -200.0\n",
      "240 -200.0\n",
      "241 -200.0\n",
      "242 -200.0\n",
      "243 -200.0\n",
      "244 -200.0\n",
      "245 -200.0\n",
      "246 -200.0\n",
      "247 -200.0\n",
      "248 -200.0\n",
      "249 -200.0\n",
      "250 -200.0\n",
      "251 -200.0\n",
      "252 -200.0\n",
      "253 -200.0\n",
      "254 -200.0\n",
      "255 -200.0\n",
      "256 -200.0\n",
      "257 -200.0\n",
      "258 -200.0\n",
      "259 -200.0\n",
      "260 -200.0\n",
      "261 -200.0\n",
      "262 -200.0\n",
      "263 -200.0\n",
      "264 -200.0\n",
      "265 -200.0\n",
      "266 -200.0\n",
      "267 -200.0\n",
      "268 -200.0\n",
      "269 -200.0\n",
      "270 -200.0\n",
      "271 -200.0\n",
      "272 -200.0\n",
      "273 -200.0\n",
      "274 -200.0\n",
      "275 -200.0\n",
      "276 -200.0\n",
      "277 -200.0\n",
      "278 -200.0\n",
      "279 -200.0\n",
      "280 -200.0\n",
      "281 -200.0\n",
      "282 -200.0\n",
      "283 -200.0\n",
      "284 -200.0\n",
      "285 -200.0\n",
      "286 -200.0\n",
      "287 -200.0\n",
      "288 -200.0\n",
      "289 -200.0\n",
      "290 -200.0\n",
      "291 -200.0\n",
      "292 -200.0\n",
      "293 -200.0\n",
      "294 -200.0\n",
      "295 -200.0\n",
      "296 -200.0\n",
      "297 -200.0\n",
      "298 -200.0\n",
      "299 -200.0\n",
      "300 -200.0\n",
      "301 -200.0\n",
      "302 -200.0\n",
      "303 -200.0\n",
      "304 -200.0\n",
      "305 -200.0\n",
      "306 -200.0\n",
      "307 -200.0\n",
      "308 -200.0\n",
      "309 -200.0\n",
      "310 -200.0\n",
      "311 -200.0\n",
      "312 -200.0\n",
      "313 -200.0\n",
      "314 -200.0\n",
      "315 -200.0\n",
      "316 -200.0\n",
      "317 -200.0\n",
      "318 -200.0\n",
      "319 -200.0\n",
      "320 -200.0\n",
      "321 -200.0\n",
      "322 -200.0\n",
      "323 -200.0\n",
      "324 -200.0\n",
      "325 -200.0\n",
      "326 -200.0\n",
      "327 -200.0\n",
      "328 -200.0\n",
      "329 -200.0\n",
      "330 -200.0\n",
      "331 -200.0\n",
      "332 -200.0\n",
      "333 -200.0\n",
      "334 -200.0\n",
      "335 -200.0\n",
      "336 -200.0\n",
      "337 -200.0\n",
      "338 -200.0\n",
      "339 -200.0\n",
      "340 -200.0\n",
      "341 -200.0\n",
      "342 -200.0\n",
      "343 -200.0\n",
      "344 -200.0\n",
      "345 -200.0\n",
      "346 -200.0\n",
      "347 -200.0\n",
      "348 -200.0\n",
      "349 -200.0\n",
      "350 -200.0\n",
      "351 -200.0\n",
      "352 -200.0\n",
      "353 -200.0\n",
      "354 -200.0\n",
      "355 -200.0\n",
      "356 -200.0\n",
      "357 -200.0\n",
      "358 -200.0\n",
      "359 -200.0\n",
      "360 -200.0\n",
      "361 -200.0\n",
      "362 -200.0\n",
      "363 -200.0\n",
      "364 -200.0\n",
      "365 -200.0\n",
      "366 -200.0\n",
      "367 -200.0\n",
      "368 -200.0\n",
      "369 -200.0\n",
      "370 -200.0\n",
      "371 -200.0\n",
      "372 -200.0\n",
      "373 -200.0\n",
      "374 -200.0\n",
      "375 -200.0\n",
      "376 -200.0\n",
      "377 -200.0\n",
      "378 -200.0\n",
      "379 -200.0\n",
      "380 -200.0\n",
      "381 -200.0\n",
      "382 -200.0\n",
      "383 -200.0\n",
      "384 -200.0\n",
      "385 -200.0\n",
      "386 -200.0\n",
      "387 -200.0\n",
      "388 -200.0\n",
      "389 -200.0\n",
      "390 -200.0\n",
      "391 -200.0\n",
      "392 -200.0\n",
      "393 -200.0\n",
      "394 -200.0\n",
      "395 -200.0\n",
      "396 -200.0\n",
      "397 -200.0\n",
      "398 -200.0\n",
      "399 -200.0\n",
      "400 -200.0\n",
      "401 -200.0\n",
      "402 -200.0\n",
      "403 -200.0\n",
      "404 -200.0\n",
      "405 -200.0\n",
      "406 -200.0\n",
      "407 -200.0\n",
      "408 -200.0\n",
      "409 -200.0\n",
      "410 -200.0\n",
      "411 -200.0\n",
      "412 -200.0\n",
      "413 -200.0\n",
      "414 -200.0\n",
      "415 -200.0\n",
      "416 -200.0\n",
      "417 -200.0\n",
      "418 -200.0\n",
      "419 -200.0\n",
      "420 -200.0\n",
      "421 -200.0\n",
      "422 -200.0\n",
      "423 -200.0\n",
      "424 -200.0\n",
      "425 -200.0\n",
      "426 -200.0\n",
      "427 -200.0\n",
      "428 -200.0\n",
      "429 -200.0\n",
      "430 -200.0\n",
      "431 -200.0\n",
      "432 -200.0\n",
      "433 -200.0\n",
      "434 -200.0\n",
      "435 -200.0\n",
      "436 -200.0\n",
      "437 -200.0\n",
      "438 -200.0\n",
      "439 -200.0\n",
      "440 -200.0\n",
      "441 -200.0\n",
      "442 -200.0\n",
      "443 -200.0\n",
      "444 -200.0\n",
      "445 -200.0\n",
      "446 -200.0\n",
      "447 -200.0\n",
      "448 -200.0\n",
      "449 -200.0\n",
      "450 -200.0\n",
      "451 -200.0\n",
      "452 -200.0\n",
      "453 -200.0\n",
      "454 -200.0\n",
      "455 -200.0\n",
      "456 -200.0\n",
      "457 -200.0\n",
      "458 -200.0\n",
      "459 -200.0\n",
      "460 -200.0\n",
      "461 -200.0\n",
      "462 -200.0\n",
      "463 -200.0\n",
      "464 -200.0\n",
      "465 -200.0\n",
      "466 -200.0\n",
      "467 -200.0\n",
      "468 -200.0\n",
      "469 -200.0\n",
      "470 -200.0\n",
      "471 -200.0\n",
      "472 -200.0\n",
      "473 -200.0\n",
      "474 -200.0\n",
      "475 -200.0\n",
      "476 -200.0\n",
      "477 -200.0\n",
      "478 -200.0\n",
      "479 -200.0\n",
      "480 -200.0\n",
      "481 -200.0\n",
      "482 -200.0\n",
      "483 -200.0\n",
      "484 -200.0\n",
      "485 -200.0\n",
      "486 -200.0\n",
      "487 -200.0\n",
      "488 -200.0\n",
      "489 -200.0\n",
      "490 -200.0\n",
      "491 -200.0\n",
      "492 -200.0\n",
      "493 -200.0\n",
      "494 -200.0\n",
      "495 -200.0\n",
      "496 -200.0\n",
      "497 -200.0\n",
      "498 -200.0\n",
      "499 -200.0\n",
      "500 -200.0\n",
      "501 -200.0\n",
      "502 -200.0\n",
      "503 -200.0\n",
      "504 -200.0\n",
      "505 -200.0\n",
      "506 -200.0\n",
      "507 -200.0\n",
      "508 -200.0\n",
      "509 -200.0\n",
      "510 -200.0\n",
      "511 -200.0\n",
      "512 -200.0\n",
      "513 -200.0\n",
      "514 -200.0\n",
      "515 -200.0\n",
      "516 -200.0\n",
      "517 -200.0\n",
      "518 -200.0\n",
      "519 -200.0\n",
      "520 -200.0\n",
      "521 -200.0\n",
      "522 -200.0\n",
      "523 -200.0\n",
      "524 -200.0\n",
      "525 -200.0\n",
      "526 -200.0\n",
      "527 -200.0\n",
      "528 -200.0\n",
      "529 -200.0\n",
      "530 -200.0\n",
      "531 -200.0\n",
      "532 -200.0\n",
      "533 -200.0\n",
      "534 -200.0\n",
      "535 -200.0\n",
      "536 -200.0\n",
      "537 -200.0\n",
      "538 -200.0\n",
      "539 -183.0\n",
      "540 -200.0\n",
      "541 -200.0\n",
      "542 -200.0\n",
      "543 -200.0\n",
      "544 -200.0\n",
      "545 -200.0\n",
      "546 -200.0\n",
      "547 -200.0\n",
      "548 -200.0\n",
      "549 -200.0\n",
      "550 -200.0\n",
      "551 -200.0\n",
      "552 -200.0\n",
      "553 -200.0\n",
      "554 -200.0\n",
      "555 -200.0\n",
      "556 -200.0\n",
      "557 -200.0\n",
      "558 -200.0\n",
      "559 -200.0\n",
      "560 -200.0\n",
      "561 -200.0\n",
      "562 -200.0\n",
      "563 -200.0\n",
      "564 -200.0\n",
      "565 -200.0\n",
      "566 -200.0\n",
      "567 -200.0\n",
      "568 -200.0\n",
      "569 -200.0\n",
      "570 -200.0\n",
      "571 -200.0\n",
      "572 -200.0\n",
      "573 -200.0\n",
      "574 -200.0\n",
      "575 -200.0\n",
      "576 -200.0\n",
      "577 -200.0\n",
      "578 -200.0\n",
      "579 -200.0\n",
      "580 -174.0\n",
      "581 -200.0\n",
      "582 -200.0\n",
      "583 -200.0\n",
      "584 -200.0\n",
      "585 -200.0\n",
      "586 -200.0\n",
      "587 -200.0\n",
      "588 -200.0\n",
      "589 -200.0\n",
      "590 -200.0\n",
      "591 -200.0\n",
      "592 -200.0\n",
      "593 -200.0\n",
      "594 -200.0\n",
      "595 -200.0\n",
      "596 -200.0\n",
      "597 -200.0\n",
      "598 -177.0\n",
      "599 -200.0\n",
      "600 -200.0\n",
      "601 -200.0\n",
      "602 -200.0\n",
      "603 -200.0\n",
      "604 -200.0\n",
      "605 -200.0\n",
      "606 -200.0\n",
      "607 -200.0\n",
      "608 -200.0\n",
      "609 -200.0\n",
      "610 -200.0\n",
      "611 -200.0\n",
      "612 -200.0\n",
      "613 -200.0\n",
      "614 -200.0\n",
      "615 -200.0\n",
      "616 -200.0\n",
      "617 -200.0\n",
      "618 -200.0\n",
      "619 -200.0\n",
      "620 -200.0\n",
      "621 -200.0\n",
      "622 -200.0\n",
      "623 -200.0\n",
      "624 -200.0\n",
      "625 -200.0\n",
      "626 -200.0\n",
      "627 -200.0\n",
      "628 -200.0\n",
      "629 -200.0\n",
      "630 -200.0\n",
      "631 -200.0\n",
      "632 -200.0\n",
      "633 -200.0\n",
      "634 -200.0\n",
      "635 -200.0\n",
      "636 -200.0\n",
      "637 -200.0\n",
      "638 -200.0\n",
      "639 -200.0\n",
      "640 -200.0\n",
      "641 -200.0\n",
      "642 -200.0\n",
      "643 -200.0\n",
      "644 -200.0\n",
      "645 -200.0\n",
      "646 -200.0\n",
      "647 -200.0\n",
      "648 -200.0\n",
      "649 -200.0\n",
      "650 -200.0\n",
      "651 -200.0\n",
      "652 -200.0\n",
      "653 -200.0\n",
      "654 -200.0\n",
      "655 -200.0\n",
      "656 -200.0\n",
      "657 -200.0\n",
      "658 -200.0\n",
      "659 -200.0\n",
      "660 -200.0\n",
      "661 -200.0\n",
      "662 -200.0\n",
      "663 -200.0\n",
      "664 -200.0\n",
      "665 -200.0\n",
      "666 -200.0\n",
      "667 -200.0\n",
      "668 -200.0\n",
      "669 -200.0\n",
      "670 -200.0\n",
      "671 -200.0\n",
      "672 -200.0\n",
      "673 -200.0\n",
      "674 -200.0\n",
      "675 -200.0\n",
      "676 -200.0\n",
      "677 -200.0\n",
      "678 -200.0\n",
      "679 -200.0\n",
      "680 -200.0\n",
      "681 -200.0\n",
      "682 -200.0\n",
      "683 -200.0\n",
      "684 -200.0\n",
      "685 -200.0\n",
      "686 -200.0\n",
      "687 -200.0\n",
      "688 -200.0\n",
      "689 -200.0\n",
      "690 -200.0\n",
      "691 -200.0\n",
      "692 -200.0\n",
      "693 -200.0\n",
      "694 -200.0\n",
      "695 -200.0\n",
      "696 -200.0\n",
      "697 -200.0\n",
      "698 -200.0\n",
      "699 -200.0\n",
      "700 -200.0\n",
      "701 -200.0\n",
      "702 -200.0\n",
      "703 -200.0\n",
      "704 -200.0\n",
      "705 -200.0\n",
      "706 -200.0\n",
      "707 -200.0\n",
      "708 -200.0\n",
      "709 -200.0\n",
      "710 -200.0\n",
      "711 -200.0\n",
      "712 -200.0\n",
      "713 -200.0\n",
      "714 -200.0\n",
      "715 -200.0\n",
      "716 -200.0\n",
      "717 -200.0\n",
      "718 -200.0\n",
      "719 -200.0\n",
      "720 -200.0\n",
      "721 -200.0\n",
      "722 -200.0\n",
      "723 -200.0\n",
      "724 -200.0\n",
      "725 -200.0\n",
      "726 -200.0\n",
      "727 -200.0\n",
      "728 -200.0\n",
      "729 -200.0\n",
      "730 -200.0\n",
      "731 -200.0\n",
      "732 -200.0\n",
      "733 -200.0\n",
      "734 -200.0\n",
      "735 -200.0\n",
      "736 -200.0\n",
      "737 -200.0\n",
      "738 -200.0\n",
      "739 -200.0\n",
      "740 -200.0\n",
      "741 -200.0\n",
      "742 -170.0\n",
      "743 -200.0\n",
      "744 -200.0\n",
      "745 -200.0\n",
      "746 -200.0\n",
      "747 -200.0\n",
      "748 -200.0\n",
      "749 -200.0\n",
      "750 -200.0\n",
      "751 -200.0\n",
      "752 -200.0\n",
      "753 -200.0\n",
      "754 -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755 -200.0\n",
      "756 -200.0\n",
      "757 -200.0\n",
      "758 -174.0\n",
      "759 -200.0\n",
      "760 -200.0\n",
      "761 -200.0\n",
      "762 -200.0\n",
      "763 -200.0\n",
      "764 -200.0\n",
      "765 -200.0\n",
      "766 -200.0\n",
      "767 -200.0\n",
      "768 -200.0\n",
      "769 -200.0\n",
      "770 -200.0\n",
      "771 -200.0\n",
      "772 -200.0\n",
      "773 -176.0\n",
      "774 -200.0\n",
      "775 -200.0\n",
      "776 -200.0\n",
      "777 -200.0\n",
      "778 -200.0\n",
      "779 -200.0\n",
      "780 -200.0\n",
      "781 -200.0\n",
      "782 -200.0\n",
      "783 -200.0\n",
      "784 -200.0\n",
      "785 -200.0\n",
      "786 -200.0\n",
      "787 -200.0\n",
      "788 -200.0\n",
      "789 -200.0\n",
      "790 -200.0\n",
      "791 -200.0\n",
      "792 -200.0\n",
      "793 -200.0\n",
      "794 -200.0\n",
      "795 -200.0\n",
      "796 -200.0\n",
      "797 -200.0\n",
      "798 -200.0\n",
      "799 -200.0\n",
      "800 -200.0\n",
      "801 -200.0\n",
      "802 -200.0\n",
      "803 -200.0\n",
      "804 -200.0\n",
      "805 -200.0\n",
      "806 -200.0\n",
      "807 -200.0\n",
      "808 -200.0\n",
      "809 -200.0\n",
      "810 -200.0\n",
      "811 -200.0\n",
      "812 -173.0\n",
      "813 -200.0\n",
      "814 -200.0\n",
      "815 -200.0\n",
      "816 -178.0\n",
      "817 -200.0\n",
      "818 -200.0\n",
      "819 -200.0\n",
      "820 -200.0\n",
      "821 -200.0\n",
      "822 -200.0\n",
      "823 -200.0\n",
      "824 -200.0\n",
      "825 -200.0\n",
      "826 -200.0\n",
      "827 -200.0\n",
      "828 -200.0\n",
      "829 -200.0\n",
      "830 -200.0\n",
      "831 -200.0\n",
      "832 -200.0\n",
      "833 -200.0\n",
      "834 -200.0\n",
      "835 -200.0\n",
      "836 -200.0\n",
      "837 -200.0\n",
      "838 -200.0\n",
      "839 -200.0\n",
      "840 -200.0\n",
      "841 -168.0\n",
      "842 -200.0\n",
      "843 -200.0\n",
      "844 -200.0\n",
      "845 -200.0\n",
      "846 -200.0\n",
      "847 -200.0\n",
      "848 -200.0\n",
      "849 -200.0\n",
      "850 -173.0\n",
      "851 -200.0\n",
      "852 -200.0\n",
      "853 -161.0\n",
      "854 -200.0\n",
      "855 -200.0\n",
      "856 -200.0\n",
      "857 -200.0\n",
      "858 -200.0\n",
      "859 -200.0\n",
      "860 -200.0\n",
      "861 -200.0\n",
      "862 -200.0\n",
      "863 -200.0\n",
      "864 -200.0\n",
      "865 -200.0\n",
      "866 -200.0\n",
      "867 -200.0\n",
      "868 -200.0\n",
      "869 -200.0\n",
      "870 -200.0\n",
      "871 -200.0\n",
      "872 -200.0\n",
      "873 -200.0\n",
      "874 -200.0\n",
      "875 -200.0\n",
      "876 -200.0\n",
      "877 -200.0\n",
      "878 -200.0\n",
      "879 -200.0\n",
      "880 -200.0\n",
      "881 -200.0\n",
      "882 -200.0\n",
      "883 -200.0\n",
      "884 -200.0\n",
      "885 -200.0\n",
      "886 -200.0\n",
      "887 -200.0\n",
      "888 -200.0\n",
      "889 -200.0\n",
      "890 -200.0\n",
      "891 -200.0\n",
      "892 -200.0\n",
      "893 -200.0\n",
      "894 -200.0\n",
      "895 -200.0\n",
      "896 -200.0\n",
      "897 -200.0\n",
      "898 -200.0\n",
      "899 -200.0\n",
      "900 -156.0\n",
      "901 -200.0\n",
      "902 -200.0\n",
      "903 -200.0\n",
      "904 -200.0\n",
      "905 -200.0\n",
      "906 -200.0\n",
      "907 -200.0\n",
      "908 -200.0\n",
      "909 -200.0\n",
      "910 -200.0\n",
      "911 -200.0\n",
      "912 -200.0\n",
      "913 -200.0\n",
      "914 -200.0\n",
      "915 -200.0\n",
      "916 -200.0\n",
      "917 -200.0\n",
      "918 -200.0\n",
      "919 -200.0\n",
      "920 -200.0\n",
      "921 -200.0\n",
      "922 -200.0\n",
      "923 -200.0\n",
      "924 -182.0\n",
      "925 -200.0\n",
      "926 -200.0\n",
      "927 -200.0\n",
      "928 -200.0\n",
      "929 -200.0\n",
      "930 -200.0\n",
      "931 -200.0\n",
      "932 -200.0\n",
      "933 -200.0\n",
      "934 -200.0\n",
      "935 -200.0\n",
      "936 -200.0\n",
      "937 -200.0\n",
      "938 -200.0\n",
      "939 -200.0\n",
      "940 -200.0\n",
      "941 -200.0\n",
      "942 -168.0\n",
      "943 -200.0\n",
      "944 -200.0\n",
      "945 -200.0\n",
      "946 -200.0\n",
      "947 -200.0\n",
      "948 -200.0\n",
      "949 -200.0\n",
      "950 -200.0\n",
      "951 -200.0\n",
      "952 -200.0\n",
      "953 -164.0\n",
      "954 -200.0\n",
      "955 -200.0\n",
      "956 -200.0\n",
      "957 -200.0\n",
      "958 -200.0\n",
      "959 -200.0\n",
      "960 -200.0\n",
      "961 -200.0\n",
      "962 -200.0\n",
      "963 -200.0\n",
      "964 -200.0\n",
      "965 -200.0\n",
      "966 -200.0\n",
      "967 -200.0\n",
      "968 -200.0\n",
      "969 -169.0\n",
      "970 -200.0\n",
      "971 -200.0\n",
      "972 -200.0\n",
      "973 -200.0\n",
      "974 -200.0\n",
      "975 -200.0\n",
      "976 -200.0\n",
      "977 -200.0\n",
      "978 -179.0\n",
      "979 -200.0\n",
      "980 -200.0\n",
      "981 -200.0\n",
      "982 -200.0\n",
      "983 -200.0\n",
      "984 -175.0\n",
      "985 -200.0\n",
      "986 -200.0\n",
      "987 -161.0\n",
      "988 -200.0\n",
      "989 -200.0\n",
      "990 -200.0\n",
      "991 -173.0\n",
      "992 -200.0\n",
      "993 -200.0\n",
      "994 -200.0\n",
      "995 -200.0\n",
      "996 -200.0\n",
      "997 -200.0\n",
      "998 -164.0\n",
      "999 -200.0\n",
      "1000 -200.0\n",
      "1001 -200.0\n",
      "1002 -200.0\n",
      "1003 -200.0\n",
      "1004 -200.0\n",
      "1005 -200.0\n",
      "1006 -171.0\n",
      "1007 -200.0\n",
      "1008 -168.0\n",
      "1009 -200.0\n",
      "1010 -200.0\n",
      "1011 -200.0\n",
      "1012 -200.0\n",
      "1013 -200.0\n",
      "1014 -160.0\n",
      "1015 -200.0\n",
      "1016 -178.0\n",
      "1017 -200.0\n",
      "1018 -165.0\n",
      "1019 -200.0\n",
      "1020 -200.0\n",
      "1021 -200.0\n",
      "1022 -200.0\n",
      "1023 -200.0\n",
      "1024 -200.0\n",
      "1025 -200.0\n",
      "1026 -176.0\n",
      "1027 -200.0\n",
      "1028 -200.0\n",
      "1029 -200.0\n",
      "1030 -200.0\n",
      "1031 -158.0\n",
      "1032 -200.0\n",
      "1033 -181.0\n",
      "1034 -200.0\n",
      "1035 -200.0\n",
      "1036 -164.0\n",
      "1037 -200.0\n",
      "1038 -177.0\n",
      "1039 -200.0\n",
      "1040 -200.0\n",
      "1041 -200.0\n",
      "1042 -200.0\n",
      "1043 -173.0\n",
      "1044 -168.0\n",
      "1045 -200.0\n",
      "1046 -200.0\n",
      "1047 -200.0\n",
      "1048 -200.0\n",
      "1049 -200.0\n",
      "1050 -200.0\n",
      "1051 -154.0\n",
      "1052 -165.0\n",
      "1053 -200.0\n",
      "1054 -200.0\n",
      "1055 -200.0\n",
      "1056 -200.0\n",
      "1057 -200.0\n",
      "1058 -200.0\n",
      "1059 -200.0\n",
      "1060 -200.0\n",
      "1061 -169.0\n",
      "1062 -200.0\n",
      "1063 -200.0\n",
      "1064 -200.0\n",
      "1065 -200.0\n",
      "1066 -200.0\n",
      "1067 -200.0\n",
      "1068 -200.0\n",
      "1069 -200.0\n",
      "1070 -200.0\n",
      "1071 -200.0\n",
      "1072 -200.0\n",
      "1073 -200.0\n",
      "1074 -200.0\n",
      "1075 -200.0\n",
      "1076 -175.0\n",
      "1077 -200.0\n",
      "1078 -200.0\n",
      "1079 -200.0\n",
      "1080 -200.0\n",
      "1081 -200.0\n",
      "1082 -200.0\n",
      "1083 -167.0\n",
      "1084 -200.0\n",
      "1085 -200.0\n",
      "1086 -200.0\n",
      "1087 -200.0\n",
      "1088 -200.0\n",
      "1089 -156.0\n",
      "1090 -200.0\n",
      "1091 -148.0\n",
      "1092 -200.0\n",
      "1093 -200.0\n",
      "1094 -186.0\n",
      "1095 -200.0\n",
      "1096 -152.0\n",
      "1097 -200.0\n",
      "1098 -200.0\n",
      "1099 -176.0\n",
      "1100 -200.0\n",
      "1101 -200.0\n",
      "1102 -200.0\n",
      "1103 -200.0\n",
      "1104 -200.0\n",
      "1105 -200.0\n",
      "1106 -200.0\n",
      "1107 -149.0\n",
      "1108 -173.0\n",
      "1109 -200.0\n",
      "1110 -200.0\n",
      "1111 -200.0\n",
      "1112 -200.0\n",
      "1113 -200.0\n",
      "1114 -200.0\n",
      "1115 -200.0\n",
      "1116 -200.0\n",
      "1117 -200.0\n",
      "1118 -200.0\n",
      "1119 -200.0\n",
      "1120 -200.0\n",
      "1121 -200.0\n",
      "1122 -200.0\n",
      "1123 -200.0\n",
      "1124 -200.0\n",
      "1125 -200.0\n",
      "1126 -200.0\n",
      "1127 -200.0\n",
      "1128 -200.0\n",
      "1129 -200.0\n",
      "1130 -200.0\n",
      "1131 -163.0\n",
      "1132 -200.0\n",
      "1133 -200.0\n",
      "1134 -200.0\n",
      "1135 -200.0\n",
      "1136 -200.0\n",
      "1137 -200.0\n",
      "1138 -157.0\n",
      "1139 -200.0\n",
      "1140 -173.0\n",
      "1141 -200.0\n",
      "1142 -183.0\n",
      "1143 -200.0\n",
      "1144 -171.0\n",
      "1145 -200.0\n",
      "1146 -200.0\n",
      "1147 -200.0\n",
      "1148 -200.0\n",
      "1149 -200.0\n",
      "1150 -164.0\n",
      "1151 -200.0\n",
      "1152 -200.0\n",
      "1153 -200.0\n",
      "1154 -200.0\n",
      "1155 -168.0\n",
      "1156 -137.0\n",
      "1157 -184.0\n",
      "1158 -200.0\n",
      "1159 -200.0\n",
      "1160 -200.0\n",
      "1161 -200.0\n",
      "1162 -200.0\n",
      "1163 -200.0\n",
      "1164 -200.0\n",
      "1165 -147.0\n",
      "1166 -200.0\n",
      "1167 -200.0\n",
      "1168 -200.0\n",
      "1169 -144.0\n",
      "1170 -200.0\n",
      "1171 -200.0\n",
      "1172 -200.0\n",
      "1173 -200.0\n",
      "1174 -200.0\n",
      "1175 -200.0\n",
      "1176 -200.0\n",
      "1177 -200.0\n",
      "1178 -200.0\n",
      "1179 -200.0\n",
      "1180 -200.0\n",
      "1181 -159.0\n",
      "1182 -200.0\n",
      "1183 -200.0\n",
      "1184 -200.0\n",
      "1185 -200.0\n",
      "1186 -200.0\n",
      "1187 -200.0\n",
      "1188 -163.0\n",
      "1189 -200.0\n",
      "1190 -200.0\n",
      "1191 -200.0\n",
      "1192 -200.0\n",
      "1193 -200.0\n",
      "1194 -200.0\n",
      "1195 -200.0\n",
      "1196 -200.0\n",
      "1197 -152.0\n",
      "1198 -200.0\n",
      "1199 -200.0\n",
      "1200 -200.0\n",
      "1201 -180.0\n",
      "1202 -200.0\n",
      "1203 -200.0\n",
      "1204 -200.0\n",
      "1205 -200.0\n",
      "1206 -200.0\n",
      "1207 -200.0\n",
      "1208 -163.0\n",
      "1209 -200.0\n",
      "1210 -200.0\n",
      "1211 -200.0\n",
      "1212 -200.0\n",
      "1213 -200.0\n",
      "1214 -200.0\n",
      "1215 -200.0\n",
      "1216 -192.0\n",
      "1217 -200.0\n",
      "1218 -166.0\n",
      "1219 -200.0\n",
      "1220 -200.0\n",
      "1221 -200.0\n",
      "1222 -200.0\n",
      "1223 -155.0\n",
      "1224 -200.0\n",
      "1225 -200.0\n",
      "1226 -200.0\n",
      "1227 -200.0\n",
      "1228 -200.0\n",
      "1229 -174.0\n",
      "1230 -200.0\n",
      "1231 -200.0\n",
      "1232 -200.0\n",
      "1233 -175.0\n",
      "1234 -200.0\n",
      "1235 -200.0\n",
      "1236 -200.0\n",
      "1237 -200.0\n",
      "1238 -200.0\n",
      "1239 -200.0\n",
      "1240 -200.0\n",
      "1241 -200.0\n",
      "1242 -200.0\n",
      "1243 -165.0\n",
      "1244 -130.0\n",
      "1245 -200.0\n",
      "1246 -200.0\n",
      "1247 -200.0\n",
      "1248 -200.0\n",
      "1249 -200.0\n",
      "1250 -200.0\n",
      "1251 -200.0\n",
      "1252 -161.0\n",
      "1253 -200.0\n",
      "1254 -200.0\n",
      "1255 -200.0\n",
      "1256 -200.0\n",
      "1257 -200.0\n",
      "1258 -200.0\n",
      "1259 -200.0\n",
      "1260 -200.0\n",
      "1261 -200.0\n",
      "1262 -200.0\n",
      "1263 -200.0\n",
      "1264 -200.0\n",
      "1265 -200.0\n",
      "1266 -200.0\n",
      "1267 -200.0\n",
      "1268 -173.0\n",
      "1269 -200.0\n",
      "1270 -200.0\n",
      "1271 -200.0\n",
      "1272 -126.0\n",
      "1273 -200.0\n",
      "1274 -200.0\n",
      "1275 -200.0\n",
      "1276 -200.0\n",
      "1277 -200.0\n",
      "1278 -200.0\n",
      "1279 -200.0\n",
      "1280 -200.0\n",
      "1281 -200.0\n",
      "1282 -157.0\n",
      "1283 -200.0\n",
      "1284 -200.0\n",
      "1285 -200.0\n",
      "1286 -200.0\n",
      "1287 -200.0\n",
      "1288 -200.0\n",
      "1289 -200.0\n",
      "1290 -200.0\n",
      "1291 -200.0\n",
      "1292 -200.0\n",
      "1293 -200.0\n",
      "1294 -200.0\n",
      "1295 -200.0\n",
      "1296 -200.0\n",
      "1297 -154.0\n",
      "1298 -200.0\n",
      "1299 -200.0\n",
      "1300 -200.0\n",
      "1301 -200.0\n",
      "1302 -200.0\n",
      "1303 -200.0\n",
      "1304 -200.0\n",
      "1305 -200.0\n",
      "1306 -200.0\n",
      "1307 -132.0\n",
      "1308 -200.0\n",
      "1309 -200.0\n",
      "1310 -200.0\n",
      "1311 -188.0\n",
      "1312 -200.0\n",
      "1313 -200.0\n",
      "1314 -194.0\n",
      "1315 -200.0\n",
      "1316 -200.0\n",
      "1317 -154.0\n",
      "1318 -200.0\n",
      "1319 -200.0\n",
      "1320 -200.0\n",
      "1321 -200.0\n",
      "1322 -200.0\n",
      "1323 -200.0\n",
      "1324 -200.0\n",
      "1325 -200.0\n",
      "1326 -200.0\n",
      "1327 -200.0\n",
      "1328 -200.0\n",
      "1329 -137.0\n",
      "1330 -177.0\n",
      "1331 -200.0\n",
      "1332 -200.0\n",
      "1333 -200.0\n",
      "1334 -200.0\n",
      "1335 -200.0\n",
      "1336 -181.0\n",
      "1337 -200.0\n",
      "1338 -197.0\n",
      "1339 -200.0\n",
      "1340 -140.0\n",
      "1341 -200.0\n",
      "1342 -200.0\n",
      "1343 -200.0\n",
      "1344 -200.0\n",
      "1345 -200.0\n",
      "1346 -200.0\n",
      "1347 -200.0\n",
      "1348 -200.0\n",
      "1349 -200.0\n",
      "1350 -133.0\n",
      "1351 -200.0\n",
      "1352 -200.0\n",
      "1353 -200.0\n",
      "1354 -200.0\n",
      "1355 -200.0\n",
      "1356 -200.0\n",
      "1357 -180.0\n",
      "1358 -200.0\n",
      "1359 -200.0\n",
      "1360 -200.0\n",
      "1361 -133.0\n",
      "1362 -175.0\n",
      "1363 -200.0\n",
      "1364 -200.0\n",
      "1365 -200.0\n",
      "1366 -200.0\n",
      "1367 -200.0\n",
      "1368 -200.0\n",
      "1369 -124.0\n",
      "1370 -200.0\n",
      "1371 -200.0\n",
      "1372 -200.0\n",
      "1373 -200.0\n",
      "1374 -186.0\n",
      "1375 -200.0\n",
      "1376 -200.0\n",
      "1377 -200.0\n",
      "1378 -132.0\n",
      "1379 -200.0\n",
      "1380 -200.0\n",
      "1381 -200.0\n",
      "1382 -198.0\n",
      "1383 -200.0\n",
      "1384 -200.0\n",
      "1385 -200.0\n",
      "1386 -200.0\n",
      "1387 -200.0\n",
      "1388 -200.0\n",
      "1389 -168.0\n",
      "1390 -200.0\n",
      "1391 -200.0\n",
      "1392 -200.0\n",
      "1393 -200.0\n",
      "1394 -135.0\n",
      "1395 -132.0\n",
      "1396 -200.0\n",
      "1397 -200.0\n",
      "1398 -200.0\n",
      "1399 -137.0\n",
      "1400 -200.0\n",
      "1401 -200.0\n",
      "1402 -200.0\n",
      "1403 -200.0\n",
      "1404 -200.0\n",
      "1405 -200.0\n",
      "1406 -200.0\n",
      "1407 -200.0\n",
      "1408 -128.0\n",
      "1409 -200.0\n",
      "1410 -179.0\n",
      "1411 -130.0\n",
      "1412 -200.0\n",
      "1413 -200.0\n",
      "1414 -200.0\n",
      "1415 -127.0\n",
      "1416 -200.0\n",
      "1417 -200.0\n",
      "1418 -200.0\n",
      "1419 -200.0\n",
      "1420 -200.0\n",
      "1421 -200.0\n",
      "1422 -200.0\n",
      "1423 -200.0\n",
      "1424 -199.0\n",
      "1425 -175.0\n",
      "1426 -174.0\n",
      "1427 -200.0\n",
      "1428 -200.0\n",
      "1429 -200.0\n",
      "1430 -200.0\n",
      "1431 -200.0\n",
      "1432 -200.0\n",
      "1433 -200.0\n",
      "1434 -200.0\n",
      "1435 -200.0\n",
      "1436 -134.0\n",
      "1437 -200.0\n",
      "1438 -167.0\n",
      "1439 -200.0\n",
      "1440 -191.0\n",
      "1441 -200.0\n",
      "1442 -200.0\n",
      "1443 -136.0\n",
      "1444 -135.0\n",
      "1445 -200.0\n",
      "1446 -200.0\n",
      "1447 -147.0\n",
      "1448 -200.0\n",
      "1449 -149.0\n",
      "1450 -142.0\n",
      "1451 -200.0\n",
      "1452 -200.0\n",
      "1453 -200.0\n",
      "1454 -200.0\n",
      "1455 -200.0\n",
      "1456 -137.0\n",
      "1457 -200.0\n",
      "1458 -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1459 -200.0\n",
      "1460 -200.0\n",
      "1461 -180.0\n",
      "1462 -200.0\n",
      "1463 -144.0\n",
      "1464 -200.0\n",
      "1465 -200.0\n",
      "1466 -200.0\n",
      "1467 -134.0\n",
      "1468 -131.0\n",
      "1469 -200.0\n",
      "1470 -149.0\n",
      "1471 -200.0\n",
      "1472 -200.0\n",
      "1473 -200.0\n",
      "1474 -149.0\n",
      "1475 -194.0\n",
      "1476 -143.0\n",
      "1477 -200.0\n",
      "1478 -200.0\n",
      "1479 -200.0\n",
      "1480 -200.0\n",
      "1481 -123.0\n",
      "1482 -200.0\n",
      "1483 -200.0\n",
      "1484 -200.0\n",
      "1485 -200.0\n",
      "1486 -200.0\n",
      "1487 -200.0\n",
      "1488 -200.0\n",
      "1489 -200.0\n",
      "1490 -200.0\n",
      "1491 -200.0\n",
      "1492 -200.0\n",
      "1493 -151.0\n",
      "1494 -200.0\n",
      "1495 -200.0\n",
      "1496 -200.0\n",
      "1497 -200.0\n",
      "1498 -200.0\n",
      "1499 -200.0\n",
      "1500 -159.0\n",
      "1501 -200.0\n",
      "1502 -200.0\n",
      "1503 -200.0\n",
      "1504 -200.0\n",
      "1505 -136.0\n",
      "1506 -200.0\n",
      "1507 -200.0\n",
      "1508 -200.0\n",
      "1509 -200.0\n",
      "1510 -200.0\n",
      "1511 -200.0\n",
      "1512 -200.0\n",
      "1513 -200.0\n",
      "1514 -200.0\n",
      "1515 -200.0\n",
      "1516 -200.0\n",
      "1517 -152.0\n",
      "1518 -200.0\n",
      "1519 -200.0\n",
      "1520 -200.0\n",
      "1521 -200.0\n",
      "1522 -200.0\n",
      "1523 -200.0\n",
      "1524 -200.0\n",
      "1525 -200.0\n",
      "1526 -133.0\n",
      "1527 -200.0\n",
      "1528 -200.0\n",
      "1529 -200.0\n",
      "1530 -200.0\n",
      "1531 -200.0\n",
      "1532 -200.0\n",
      "1533 -200.0\n",
      "1534 -127.0\n",
      "1535 -200.0\n",
      "1536 -200.0\n",
      "1537 -143.0\n",
      "1538 -200.0\n",
      "1539 -200.0\n",
      "1540 -136.0\n",
      "1541 -200.0\n",
      "1542 -131.0\n",
      "1543 -200.0\n",
      "1544 -200.0\n",
      "1545 -200.0\n",
      "1546 -200.0\n",
      "1547 -200.0\n",
      "1548 -200.0\n",
      "1549 -200.0\n",
      "1550 -200.0\n",
      "1551 -200.0\n",
      "1552 -200.0\n",
      "1553 -136.0\n",
      "1554 -200.0\n",
      "1555 -200.0\n",
      "1556 -200.0\n",
      "1557 -200.0\n",
      "1558 -183.0\n",
      "1559 -135.0\n",
      "1560 -200.0\n",
      "1561 -200.0\n",
      "1562 -200.0\n",
      "1563 -200.0\n",
      "1564 -118.0\n",
      "1565 -200.0\n",
      "1566 -200.0\n",
      "1567 -165.0\n",
      "1568 -200.0\n",
      "1569 -137.0\n",
      "1570 -133.0\n",
      "1571 -200.0\n",
      "1572 -200.0\n",
      "1573 -162.0\n",
      "1574 -200.0\n",
      "1575 -118.0\n",
      "1576 -200.0\n",
      "1577 -200.0\n",
      "1578 -185.0\n",
      "1579 -132.0\n",
      "1580 -200.0\n",
      "1581 -200.0\n",
      "1582 -200.0\n",
      "1583 -200.0\n",
      "1584 -200.0\n",
      "1585 -200.0\n",
      "1586 -200.0\n",
      "1587 -200.0\n",
      "1588 -200.0\n",
      "1589 -200.0\n",
      "1590 -153.0\n",
      "1591 -200.0\n",
      "1592 -200.0\n",
      "1593 -200.0\n",
      "1594 -200.0\n",
      "1595 -200.0\n",
      "1596 -165.0\n",
      "1597 -200.0\n",
      "1598 -200.0\n",
      "1599 -200.0\n",
      "1600 -200.0\n",
      "1601 -200.0\n",
      "1602 -195.0\n",
      "1603 -200.0\n",
      "1604 -200.0\n",
      "1605 -200.0\n",
      "1606 -200.0\n",
      "1607 -165.0\n",
      "1608 -156.0\n",
      "1609 -200.0\n",
      "1610 -200.0\n",
      "1611 -200.0\n",
      "1612 -170.0\n",
      "1613 -200.0\n",
      "1614 -135.0\n",
      "1615 -200.0\n",
      "1616 -200.0\n",
      "1617 -173.0\n",
      "1618 -133.0\n",
      "1619 -200.0\n",
      "1620 -200.0\n",
      "1621 -200.0\n",
      "1622 -200.0\n",
      "1623 -179.0\n",
      "1624 -200.0\n",
      "1625 -200.0\n",
      "1626 -123.0\n",
      "1627 -200.0\n",
      "1628 -200.0\n",
      "1629 -171.0\n",
      "1630 -159.0\n",
      "1631 -200.0\n",
      "1632 -200.0\n",
      "1633 -182.0\n",
      "1634 -200.0\n",
      "1635 -200.0\n",
      "1636 -188.0\n",
      "1637 -200.0\n",
      "1638 -148.0\n",
      "1639 -200.0\n",
      "1640 -200.0\n",
      "1641 -200.0\n",
      "1642 -162.0\n",
      "1643 -200.0\n",
      "1644 -200.0\n",
      "1645 -200.0\n",
      "1646 -200.0\n",
      "1647 -176.0\n",
      "1648 -172.0\n",
      "1649 -200.0\n",
      "1650 -200.0\n",
      "1651 -194.0\n",
      "1652 -200.0\n",
      "1653 -200.0\n",
      "1654 -186.0\n",
      "1655 -200.0\n",
      "1656 -200.0\n",
      "1657 -200.0\n",
      "1658 -200.0\n",
      "1659 -152.0\n",
      "1660 -200.0\n",
      "1661 -169.0\n",
      "1662 -185.0\n",
      "1663 -200.0\n",
      "1664 -189.0\n",
      "1665 -200.0\n",
      "1666 -191.0\n",
      "1667 -200.0\n",
      "1668 -200.0\n",
      "1669 -200.0\n",
      "1670 -200.0\n",
      "1671 -200.0\n",
      "1672 -200.0\n",
      "1673 -162.0\n",
      "1674 -193.0\n",
      "1675 -200.0\n",
      "1676 -181.0\n",
      "1677 -200.0\n",
      "1678 -200.0\n",
      "1679 -133.0\n",
      "1680 -200.0\n",
      "1681 -200.0\n",
      "1682 -151.0\n",
      "1683 -200.0\n",
      "1684 -200.0\n",
      "1685 -128.0\n",
      "1686 -200.0\n",
      "1687 -200.0\n",
      "1688 -179.0\n",
      "1689 -147.0\n",
      "1690 -200.0\n",
      "1691 -166.0\n",
      "1692 -200.0\n",
      "1693 -200.0\n",
      "1694 -124.0\n",
      "1695 -200.0\n",
      "1696 -200.0\n",
      "1697 -200.0\n",
      "1698 -200.0\n",
      "1699 -200.0\n",
      "1700 -200.0\n",
      "1701 -200.0\n",
      "1702 -200.0\n",
      "1703 -200.0\n",
      "1704 -200.0\n",
      "1705 -135.0\n",
      "1706 -200.0\n",
      "1707 -161.0\n",
      "1708 -200.0\n",
      "1709 -200.0\n",
      "1710 -170.0\n",
      "1711 -200.0\n",
      "1712 -172.0\n",
      "1713 -200.0\n",
      "1714 -200.0\n",
      "1715 -188.0\n",
      "1716 -200.0\n",
      "1717 -200.0\n",
      "1718 -200.0\n",
      "1719 -200.0\n",
      "1720 -172.0\n",
      "1721 -200.0\n",
      "1722 -200.0\n",
      "1723 -200.0\n",
      "1724 -200.0\n",
      "1725 -200.0\n",
      "1726 -151.0\n",
      "1727 -200.0\n",
      "1728 -200.0\n",
      "1729 -200.0\n",
      "1730 -200.0\n",
      "1731 -200.0\n",
      "1732 -169.0\n",
      "1733 -200.0\n",
      "1734 -200.0\n",
      "1735 -124.0\n",
      "1736 -200.0\n",
      "1737 -200.0\n",
      "1738 -200.0\n",
      "1739 -167.0\n",
      "1740 -200.0\n",
      "1741 -200.0\n",
      "1742 -197.0\n",
      "1743 -163.0\n",
      "1744 -200.0\n",
      "1745 -200.0\n",
      "1746 -200.0\n",
      "1747 -200.0\n",
      "1748 -171.0\n",
      "1749 -175.0\n",
      "1750 -200.0\n",
      "1751 -179.0\n",
      "1752 -200.0\n",
      "1753 -175.0\n",
      "1754 -200.0\n",
      "1755 -200.0\n",
      "1756 -200.0\n",
      "1757 -200.0\n",
      "1758 -200.0\n",
      "1759 -200.0\n",
      "1760 -200.0\n",
      "1761 -200.0\n",
      "1762 -200.0\n",
      "1763 -200.0\n",
      "1764 -200.0\n",
      "1765 -200.0\n",
      "1766 -200.0\n",
      "1767 -200.0\n",
      "1768 -133.0\n",
      "1769 -200.0\n",
      "1770 -125.0\n",
      "1771 -200.0\n",
      "1772 -200.0\n",
      "1773 -200.0\n",
      "1774 -200.0\n",
      "1775 -200.0\n",
      "1776 -200.0\n",
      "1777 -200.0\n",
      "1778 -200.0\n",
      "1779 -154.0\n",
      "1780 -200.0\n",
      "1781 -200.0\n",
      "1782 -200.0\n",
      "1783 -154.0\n",
      "1784 -200.0\n",
      "1785 -200.0\n",
      "1786 -161.0\n",
      "1787 -184.0\n",
      "1788 -200.0\n",
      "1789 -170.0\n",
      "1790 -200.0\n",
      "1791 -176.0\n",
      "1792 -200.0\n",
      "1793 -200.0\n",
      "1794 -200.0\n",
      "1795 -191.0\n",
      "1796 -200.0\n",
      "1797 -174.0\n",
      "1798 -200.0\n",
      "1799 -170.0\n",
      "1800 -161.0\n",
      "1801 -170.0\n",
      "1802 -200.0\n",
      "1803 -200.0\n",
      "1804 -200.0\n",
      "1805 -200.0\n",
      "1806 -173.0\n",
      "1807 -200.0\n",
      "1808 -156.0\n",
      "1809 -200.0\n",
      "1810 -200.0\n",
      "1811 -172.0\n",
      "1812 -200.0\n",
      "1813 -200.0\n",
      "1814 -188.0\n",
      "1815 -200.0\n",
      "1816 -200.0\n",
      "1817 -200.0\n",
      "1818 -154.0\n",
      "1819 -174.0\n",
      "1820 -187.0\n",
      "1821 -200.0\n",
      "1822 -200.0\n",
      "1823 -151.0\n",
      "1824 -200.0\n",
      "1825 -176.0\n",
      "1826 -192.0\n",
      "1827 -200.0\n",
      "1828 -200.0\n",
      "1829 -159.0\n",
      "1830 -200.0\n",
      "1831 -161.0\n",
      "1832 -200.0\n",
      "1833 -187.0\n",
      "1834 -200.0\n",
      "1835 -200.0\n",
      "1836 -165.0\n",
      "1837 -159.0\n",
      "1838 -176.0\n",
      "1839 -179.0\n",
      "1840 -200.0\n",
      "1841 -178.0\n",
      "1842 -132.0\n",
      "1843 -200.0\n",
      "1844 -200.0\n",
      "1845 -200.0\n",
      "1846 -200.0\n",
      "1847 -175.0\n",
      "1848 -171.0\n",
      "1849 -200.0\n",
      "1850 -200.0\n",
      "1851 -166.0\n",
      "1852 -146.0\n",
      "1853 -200.0\n",
      "1854 -158.0\n",
      "1855 -186.0\n",
      "1856 -200.0\n",
      "1857 -172.0\n",
      "1858 -200.0\n",
      "1859 -200.0\n",
      "1860 -200.0\n",
      "1861 -181.0\n",
      "1862 -200.0\n",
      "1863 -200.0\n",
      "1864 -200.0\n",
      "1865 -200.0\n",
      "1866 -200.0\n",
      "1867 -200.0\n",
      "1868 -141.0\n",
      "1869 -200.0\n",
      "1870 -159.0\n",
      "1871 -200.0\n",
      "1872 -179.0\n",
      "1873 -200.0\n",
      "1874 -200.0\n",
      "1875 -127.0\n",
      "1876 -200.0\n",
      "1877 -124.0\n",
      "1878 -200.0\n",
      "1879 -161.0\n",
      "1880 -200.0\n",
      "1881 -187.0\n",
      "1882 -200.0\n",
      "1883 -184.0\n",
      "1884 -179.0\n",
      "1885 -177.0\n",
      "1886 -200.0\n",
      "1887 -200.0\n",
      "1888 -200.0\n",
      "1889 -180.0\n",
      "1890 -170.0\n",
      "1891 -200.0\n",
      "1892 -197.0\n",
      "1893 -200.0\n",
      "1894 -200.0\n",
      "1895 -200.0\n",
      "1896 -200.0\n",
      "1897 -144.0\n",
      "1898 -200.0\n",
      "1899 -193.0\n",
      "1900 -200.0\n",
      "1901 -168.0\n",
      "1902 -178.0\n",
      "1903 -200.0\n",
      "1904 -179.0\n",
      "1905 -200.0\n",
      "1906 -181.0\n",
      "1907 -200.0\n",
      "1908 -140.0\n",
      "1909 -131.0\n",
      "1910 -200.0\n",
      "1911 -200.0\n",
      "1912 -174.0\n",
      "1913 -173.0\n",
      "1914 -172.0\n",
      "1915 -176.0\n",
      "1916 -179.0\n",
      "1917 -190.0\n",
      "1918 -155.0\n",
      "1919 -200.0\n",
      "1920 -200.0\n",
      "1921 -200.0\n",
      "1922 -166.0\n",
      "1923 -178.0\n",
      "1924 -200.0\n",
      "1925 -133.0\n",
      "1926 -181.0\n",
      "1927 -101.0\n",
      "1928 -164.0\n",
      "1929 -180.0\n",
      "1930 -200.0\n",
      "1931 -176.0\n",
      "1932 -200.0\n",
      "1933 -200.0\n",
      "1934 -163.0\n",
      "1935 -200.0\n",
      "1936 -135.0\n",
      "1937 -200.0\n",
      "1938 -177.0\n",
      "1939 -200.0\n",
      "1940 -162.0\n",
      "1941 -185.0\n",
      "1942 -200.0\n",
      "1943 -200.0\n",
      "1944 -200.0\n",
      "1945 -178.0\n",
      "1946 -179.0\n",
      "1947 -200.0\n",
      "1948 -171.0\n",
      "1949 -187.0\n",
      "1950 -183.0\n",
      "1951 -200.0\n",
      "1952 -182.0\n",
      "1953 -187.0\n",
      "1954 -200.0\n",
      "1955 -178.0\n",
      "1956 -162.0\n",
      "1957 -176.0\n",
      "1958 -188.0\n",
      "1959 -200.0\n",
      "1960 -165.0\n",
      "1961 -200.0\n",
      "1962 -173.0\n",
      "1963 -200.0\n",
      "1964 -174.0\n",
      "1965 -187.0\n",
      "1966 -128.0\n",
      "1967 -165.0\n",
      "1968 -176.0\n",
      "1969 -200.0\n",
      "1970 -134.0\n",
      "1971 -114.0\n",
      "1972 -163.0\n",
      "1973 -159.0\n",
      "1974 -169.0\n",
      "1975 -200.0\n",
      "1976 -200.0\n",
      "1977 -188.0\n",
      "1978 -179.0\n",
      "1979 -200.0\n",
      "1980 -173.0\n",
      "1981 -200.0\n",
      "1982 -200.0\n",
      "1983 -176.0\n",
      "1984 -200.0\n",
      "1985 -200.0\n",
      "1986 -200.0\n",
      "1987 -200.0\n",
      "1988 -200.0\n",
      "1989 -164.0\n",
      "1990 -169.0\n",
      "1991 -200.0\n",
      "1992 -200.0\n",
      "1993 -167.0\n",
      "1994 -134.0\n",
      "1995 -200.0\n",
      "1996 -163.0\n",
      "1997 -178.0\n",
      "1998 -96.0\n",
      "1999 -184.0\n"
     ]
    }
   ],
   "source": [
    "w_1 = np.zeros(8*8*8 + 3)\n",
    "w_2 = np.zeros(8*8*8 + 3)\n",
    "eps = 1.0\n",
    "decay = 0.99\n",
    "alpha = 1e-3\n",
    "done = False\n",
    "gamma = 0.995\n",
    "r = -1\n",
    "params_1 = []\n",
    "params_2 = []\n",
    "rewards = []\n",
    "for k in range(2000):\n",
    "    s = env.reset()\n",
    "    a = 1\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        t = np.random.random()\n",
    "        if t < eps:\n",
    "                a = np.random.choice([0,1,2])\n",
    "  \n",
    "        else:\n",
    "                a = np.argmax([X(s,b)@(w_1 + w_2) for b in [0,1,2]])\n",
    "            \n",
    "        s_prime,r,done,_ = env.step(a)\n",
    "        \n",
    "        coin = np.random.choice([0,1])\n",
    "        \n",
    "        if coin == 0:\n",
    "            a_prime = np.argmax([X(s_prime,a) @ w_1 for a in [0,1,2]])\n",
    "            w_1 = w_1 + alpha * (r + gamma*(X(s_prime,a_prime) @ w_2) - X(s,a) @ w_1) * X(s,a)\n",
    "            \n",
    "        else:\n",
    "            a_prime = np.argmax([X(s_prime,a) @ w_2 for a in [0,1,2]])\n",
    "            w_2 = w_2 + alpha * (r + gamma*(X(s_prime,a_prime) @ w_1) - X(s,a) @ w_2) * X(s,a) \n",
    "        \n",
    "        \n",
    "        s = s_prime\n",
    "        eps = eps * decay\n",
    "        total_reward += r\n",
    "        \n",
    "#         env.render()\n",
    "    params_1.append(w_1)\n",
    "    params_2.append(w_2)\n",
    "    print(k , total_reward)\n",
    "    rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,n_hid):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_hid = n_hid\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_dim,n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid,out_dim)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        y =self.fc2(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(s,a,r,s_next,done):\n",
    "    memory.append((s,a,r,s_next,done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(2,3,32)\n",
    "target_model = DQN(2,3,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1.0\n",
    "batch_size = 64\n",
    "gamma = 0.95\n",
    "\n",
    "epsi_high = 0.9\n",
    "epsi_low = 0.05\n",
    "decay = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(state):\n",
    "        if (np.random.random() < eps):\n",
    "                action = np.random.choice([0,1,2])\n",
    "        else:\n",
    "                action = torch.argmax(dqn(torch.FloatTensor(state).unsqueeze(0)),dim=1).detach().numpy().item()\n",
    "                \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.SmoothL1Loss()\n",
    "opt = Adam(dqn.parameters(),lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(memory,batch_size):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        samples = random.sample(memory,batch_size)\n",
    "        \n",
    "        \n",
    "        s,a,r,s_next,d = (zip(*samples))\n",
    "        \n",
    "        s = torch.FloatTensor(s)\n",
    "        a = torch.LongTensor(a)\n",
    "        r = torch.FloatTensor(r)\n",
    "        s_next = torch.FloatTensor(s_next)\n",
    "        d = torch.FloatTensor(d)\n",
    "        \n",
    "        target_q = (r + gamma*target_model(s_next).max(dim=1)[0].detach()*(1 - d)).unsqueeze(1)\n",
    "        policy_q = dqn(s).gather(1,a.unsqueeze(1))\n",
    "        L = F.smooth_l1_loss(policy_q,target_q)\n",
    "        L.backward()\n",
    "        opt.step()\n",
    "        return L.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/2000, total reward: -200.0,e: 1.0)\n",
      "updated target model\n",
      "episode: 1/2000, total reward: -200.0,e: 0.9)\n",
      "updated target model\n",
      "episode: 2/2000, total reward: -200.0,e: 0.81)\n",
      "updated target model\n",
      "episode: 3/2000, total reward: -200.0,e: 0.73)\n",
      "updated target model\n",
      "episode: 4/2000, total reward: -200.0,e: 0.66)\n",
      "updated target model\n",
      "episode: 5/2000, total reward: -200.0,e: 0.59)\n",
      "updated target model\n",
      "episode: 6/2000, total reward: -200.0,e: 0.53)\n",
      "updated target model\n",
      "episode: 7/2000, total reward: -200.0,e: 0.48)\n",
      "updated target model\n",
      "episode: 8/2000, total reward: -200.0,e: 0.43)\n",
      "updated target model\n",
      "episode: 9/2000, total reward: -200.0,e: 0.39)\n",
      "updated target model\n",
      "episode: 10/2000, total reward: -200.0,e: 0.35)\n",
      "updated target model\n",
      "episode: 11/2000, total reward: -200.0,e: 0.31)\n",
      "updated target model\n",
      "episode: 12/2000, total reward: -200.0,e: 0.28)\n",
      "updated target model\n",
      "episode: 13/2000, total reward: -200.0,e: 0.25)\n",
      "updated target model\n",
      "episode: 14/2000, total reward: -200.0,e: 0.23)\n",
      "updated target model\n",
      "episode: 15/2000, total reward: -200.0,e: 0.21)\n",
      "updated target model\n",
      "episode: 16/2000, total reward: -200.0,e: 0.19)\n",
      "updated target model\n",
      "episode: 17/2000, total reward: -200.0,e: 0.17)\n",
      "updated target model\n",
      "episode: 18/2000, total reward: -200.0,e: 0.15)\n",
      "updated target model\n",
      "episode: 19/2000, total reward: -200.0,e: 0.14)\n",
      "updated target model\n",
      "episode: 20/2000, total reward: -200.0,e: 0.12)\n",
      "updated target model\n",
      "episode: 21/2000, total reward: -200.0,e: 0.11)\n",
      "updated target model\n",
      "episode: 22/2000, total reward: -200.0,e: 0.098)\n",
      "updated target model\n",
      "episode: 23/2000, total reward: -200.0,e: 0.089)\n",
      "updated target model\n",
      "episode: 24/2000, total reward: -200.0,e: 0.08)\n",
      "updated target model\n",
      "episode: 25/2000, total reward: -200.0,e: 0.072)\n",
      "updated target model\n",
      "episode: 26/2000, total reward: -200.0,e: 0.065)\n",
      "updated target model\n",
      "episode: 27/2000, total reward: -200.0,e: 0.058)\n",
      "updated target model\n",
      "episode: 28/2000, total reward: -200.0,e: 0.052)\n",
      "updated target model\n",
      "episode: 29/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 30/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 31/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 32/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 33/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 34/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 35/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 36/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 37/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 38/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 39/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 40/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 41/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 42/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 43/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 44/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 45/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 46/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 47/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 48/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 49/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 50/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 51/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 52/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 53/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 54/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 55/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 56/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 57/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 58/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 59/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 60/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 61/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 62/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 63/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 64/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 65/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 66/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 67/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 68/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 69/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 70/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 71/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 72/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 73/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 74/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 75/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 76/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 77/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 78/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 79/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 80/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 81/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 82/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 83/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 84/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 85/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 86/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 87/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 88/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 89/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 90/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 91/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 92/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 93/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 94/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 95/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 96/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 97/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 98/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 99/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 100/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 101/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 102/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 103/2000, total reward: -176.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 104/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 105/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 106/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 107/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 108/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 109/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 110/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 111/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 112/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 113/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 114/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 115/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 116/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 117/2000, total reward: -200.0,e: 0.047)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 118/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 119/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 120/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 121/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 122/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 123/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 124/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 125/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 126/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 127/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 128/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 129/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 130/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 131/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 132/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 133/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 134/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 135/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 136/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 137/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 138/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 139/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 140/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 141/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 142/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 143/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 144/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 145/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 146/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 147/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 148/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 149/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 150/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 151/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 152/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 153/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 154/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 155/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 156/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 157/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 158/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 159/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 160/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 161/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 162/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 163/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 164/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 165/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 166/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 167/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 168/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 169/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 170/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 171/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 172/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 173/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 174/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 175/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 176/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 177/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 178/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 179/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 180/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 181/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 182/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 183/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 184/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 185/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 186/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 187/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 188/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 189/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 190/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 191/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 192/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 193/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 194/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 195/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 196/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 197/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 198/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 199/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 200/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 201/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 202/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 203/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 204/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 205/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 206/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 207/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 208/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 209/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 210/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 211/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 212/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 213/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 214/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 215/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 216/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 217/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 218/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 219/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 220/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 221/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 222/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 223/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 224/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 225/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 226/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 227/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 228/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 229/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 230/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 231/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 232/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 233/2000, total reward: -200.0,e: 0.047)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 234/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 235/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 236/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 237/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 238/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 239/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 240/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 241/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 242/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 243/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 244/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 245/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 246/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 247/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 248/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 249/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 250/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 251/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 252/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 253/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 254/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 255/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 256/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 257/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 258/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 259/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 260/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 261/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 262/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 263/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 264/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 265/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 266/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 267/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 268/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 269/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 270/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 271/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 272/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 273/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 274/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 275/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 276/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 277/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 278/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 279/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 280/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 281/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 282/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 283/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 284/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 285/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 286/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 287/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 288/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 289/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 290/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 291/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 292/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 293/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 294/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 295/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 296/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 297/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 298/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 299/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 300/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 301/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 302/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 303/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 304/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 305/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 306/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 307/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 308/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 309/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 310/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 311/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 312/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 313/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 314/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 315/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 316/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 317/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 318/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 319/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 320/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 321/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 322/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 323/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 324/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 325/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 326/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 327/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 328/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 329/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 330/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 331/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 332/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 333/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 334/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 335/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 336/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 337/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 338/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 339/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 340/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 341/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 342/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 343/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 344/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 345/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 346/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 347/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 348/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 349/2000, total reward: -200.0,e: 0.047)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 350/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 351/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 352/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 353/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 354/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 355/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 356/2000, total reward: -168.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 357/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 358/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 359/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 360/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 361/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 362/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 363/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 364/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 365/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 366/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 367/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 368/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 369/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 370/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 371/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 372/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 373/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 374/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 375/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 376/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 377/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 378/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 379/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 380/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 381/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 382/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 383/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 384/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 385/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 386/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 387/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 388/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 389/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 390/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 391/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 392/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 393/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 394/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 395/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 396/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 397/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 398/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 399/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 400/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 401/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 402/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 403/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 404/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 405/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 406/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 407/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 408/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 409/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 410/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 411/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 412/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 413/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 414/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 415/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 416/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 417/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 418/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 419/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 420/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 421/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 422/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 423/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 424/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 425/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 426/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 427/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 428/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 429/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 430/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 431/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 432/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 433/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 434/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 435/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 436/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 437/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 438/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 439/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 440/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 441/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 442/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 443/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 444/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 445/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 446/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 447/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 448/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 449/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 450/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 451/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 452/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 453/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 454/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 455/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 456/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 457/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 458/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 459/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 460/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 461/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 462/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 463/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 464/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 465/2000, total reward: -200.0,e: 0.047)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 466/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 467/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 468/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 469/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 470/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 471/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 472/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 473/2000, total reward: -131.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 474/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 475/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 476/2000, total reward: -168.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 477/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 478/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 479/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 480/2000, total reward: -163.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 481/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 482/2000, total reward: -172.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 483/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 484/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 485/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 486/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 487/2000, total reward: -196.0,e: 0.047)\n",
      "episode: 488/2000, total reward: -192.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 489/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 490/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 491/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 492/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 493/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 494/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 495/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 496/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 497/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 498/2000, total reward: -168.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 499/2000, total reward: -171.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 500/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 501/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 502/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 503/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 504/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 505/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 506/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 507/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 508/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 509/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 510/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 511/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 512/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 513/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 514/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 515/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 516/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 517/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 518/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 519/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 520/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 521/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 522/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 523/2000, total reward: -167.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 524/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 525/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 526/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 527/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 528/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 529/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 530/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 531/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 532/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 533/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 534/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 535/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 536/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 537/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 538/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 539/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 540/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 541/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 542/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 543/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 544/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 545/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 546/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 547/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 548/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 549/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 550/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 551/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 552/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 553/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 554/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 555/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 556/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 557/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 558/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 559/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 560/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 561/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 562/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 563/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 564/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 565/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 566/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 567/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 568/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 569/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 570/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 571/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 572/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 573/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 574/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 575/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 576/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 577/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 578/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 579/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 580/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 581/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 582/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 583/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 584/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 585/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 586/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 587/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 588/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 589/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 590/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 591/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 592/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 593/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 594/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 595/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 596/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 597/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 598/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 599/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 600/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 601/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 602/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 603/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 604/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 605/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 606/2000, total reward: -166.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 607/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 608/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 609/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 610/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 611/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 612/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 613/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 614/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 615/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 616/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 617/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 618/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 619/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 620/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 621/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 622/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 623/2000, total reward: -196.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 624/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 625/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 626/2000, total reward: -162.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 627/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 628/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 629/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 630/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 631/2000, total reward: -182.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 632/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 633/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 634/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 635/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 636/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 637/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 638/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 639/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 640/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 641/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 642/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 643/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 644/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 645/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 646/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 647/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 648/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 649/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 650/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 651/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 652/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 653/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 654/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 655/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 656/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 657/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 658/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 659/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 660/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 661/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 662/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 663/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 664/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 665/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 666/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 667/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 668/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 669/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 670/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 671/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 672/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 673/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 674/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 675/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 676/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 677/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 678/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 679/2000, total reward: -197.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 680/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 681/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 682/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 683/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 684/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 685/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 686/2000, total reward: -152.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 687/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 688/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 689/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 690/2000, total reward: -169.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 691/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 692/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 693/2000, total reward: -157.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 694/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 695/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 696/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 697/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 698/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 699/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 700/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 701/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 702/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 703/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 704/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 705/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 706/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 707/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 708/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 709/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 710/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 711/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 712/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 713/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 714/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 715/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 716/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 717/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 718/2000, total reward: -168.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 719/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 720/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 721/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 722/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 723/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 724/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 725/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 726/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 727/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 728/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 729/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 730/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 731/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 732/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 733/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 734/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 735/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 736/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 737/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 738/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 739/2000, total reward: -186.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 740/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 741/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 742/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 743/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 744/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 745/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 746/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 747/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 748/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 749/2000, total reward: -195.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 750/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 751/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 752/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 753/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 754/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 755/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 756/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 757/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 758/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 759/2000, total reward: -173.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 760/2000, total reward: -171.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 761/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 762/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 763/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 764/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 765/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 766/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 767/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 768/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 769/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 770/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 771/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 772/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 773/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 774/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 775/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 776/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 777/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 778/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 779/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 780/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 781/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 782/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 783/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 784/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 785/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 786/2000, total reward: -161.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 787/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 788/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 789/2000, total reward: -148.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 790/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 791/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 792/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 793/2000, total reward: -146.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 794/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 795/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 796/2000, total reward: -187.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 797/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 798/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 799/2000, total reward: -196.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 800/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 801/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 802/2000, total reward: -159.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 803/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 804/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 805/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 806/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 807/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 808/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 809/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 810/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 811/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 812/2000, total reward: -154.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 813/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 814/2000, total reward: -200.0,e: 0.047)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 815/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 816/2000, total reward: -155.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 817/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 818/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 819/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 820/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 821/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 822/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 823/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 824/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 825/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 826/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 827/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 828/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 829/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 830/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 831/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 832/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 833/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 834/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 835/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 836/2000, total reward: -169.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 837/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 838/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 839/2000, total reward: -159.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 840/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 841/2000, total reward: -149.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 842/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 843/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 844/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 845/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 846/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 847/2000, total reward: -157.0,e: 0.047)\n",
      "episode: 848/2000, total reward: -185.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 849/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 850/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 851/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 852/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 853/2000, total reward: -186.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 854/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 855/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 856/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 857/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 858/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 859/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 860/2000, total reward: -176.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 861/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 862/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 863/2000, total reward: -152.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 864/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 865/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 866/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 867/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 868/2000, total reward: -197.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 869/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 870/2000, total reward: -154.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 871/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 872/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 873/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 874/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 875/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 876/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 877/2000, total reward: -191.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 878/2000, total reward: -186.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 879/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 880/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 881/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 882/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 883/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 884/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 885/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 886/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 887/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 888/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 889/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 890/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 891/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 892/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 893/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 894/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 895/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 896/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 897/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 898/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 899/2000, total reward: -153.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 900/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 901/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 902/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 903/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 904/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 905/2000, total reward: -141.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 906/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 907/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 908/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 909/2000, total reward: -141.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 910/2000, total reward: -199.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 911/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 912/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 913/2000, total reward: -145.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 914/2000, total reward: -198.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 915/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 916/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 917/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 918/2000, total reward: -144.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 919/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 920/2000, total reward: -140.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 921/2000, total reward: -172.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 922/2000, total reward: -196.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 923/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 924/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 925/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 926/2000, total reward: -135.0,e: 0.047)\n",
      "episode: 927/2000, total reward: -139.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 928/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 929/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 930/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 931/2000, total reward: -192.0,e: 0.047)\n",
      "updated target model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 932/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 933/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 934/2000, total reward: -150.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 935/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 936/2000, total reward: -172.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 937/2000, total reward: -173.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 938/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 939/2000, total reward: -152.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 940/2000, total reward: -176.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 941/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 942/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 943/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 944/2000, total reward: -151.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 945/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 946/2000, total reward: -178.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 947/2000, total reward: -182.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 948/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 949/2000, total reward: -164.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 950/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 951/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 952/2000, total reward: -149.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 953/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 954/2000, total reward: -164.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 955/2000, total reward: -175.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 956/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 957/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 958/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 959/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 960/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 961/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 962/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 963/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 964/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 965/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 966/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 967/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 968/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 969/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 970/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 971/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 972/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 973/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 974/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 975/2000, total reward: -164.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 976/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 977/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 978/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 979/2000, total reward: -155.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 980/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 981/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 982/2000, total reward: -143.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 983/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 984/2000, total reward: -172.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 985/2000, total reward: -170.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 986/2000, total reward: -156.0,e: 0.047)\n",
      "episode: 987/2000, total reward: -143.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 988/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 989/2000, total reward: -179.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 990/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 991/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 992/2000, total reward: -184.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 993/2000, total reward: -176.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 994/2000, total reward: -147.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 995/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 996/2000, total reward: -146.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 997/2000, total reward: -161.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 998/2000, total reward: -158.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 999/2000, total reward: -184.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1000/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1001/2000, total reward: -162.0,e: 0.047)\n",
      "episode: 1002/2000, total reward: -160.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1003/2000, total reward: -170.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1004/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1005/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1006/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1007/2000, total reward: -143.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1008/2000, total reward: -185.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1009/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1010/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1011/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1012/2000, total reward: -169.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1013/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1014/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1015/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1016/2000, total reward: -196.0,e: 0.047)\n",
      "episode: 1017/2000, total reward: -190.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1018/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1019/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1020/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1021/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1022/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1023/2000, total reward: -196.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1024/2000, total reward: -139.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1025/2000, total reward: -195.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1026/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1027/2000, total reward: -192.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1028/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1029/2000, total reward: -199.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1030/2000, total reward: -141.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1031/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1032/2000, total reward: -141.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1033/2000, total reward: -140.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1034/2000, total reward: -138.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1035/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1036/2000, total reward: -140.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1037/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1038/2000, total reward: -147.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1039/2000, total reward: -151.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1040/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1041/2000, total reward: -143.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1042/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1043/2000, total reward: -143.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1044/2000, total reward: -139.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1045/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1046/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1047/2000, total reward: -147.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1048/2000, total reward: -173.0,e: 0.047)\n",
      "episode: 1049/2000, total reward: -141.0,e: 0.047)\n",
      "updated target model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1050/2000, total reward: -140.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1051/2000, total reward: -139.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1052/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1053/2000, total reward: -139.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1054/2000, total reward: -138.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1055/2000, total reward: -143.0,e: 0.047)\n",
      "episode: 1056/2000, total reward: -139.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1057/2000, total reward: -147.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1058/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1059/2000, total reward: -147.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1060/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1061/2000, total reward: -146.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1062/2000, total reward: -142.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1063/2000, total reward: -147.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1064/2000, total reward: -167.0,e: 0.047)\n",
      "episode: 1065/2000, total reward: -149.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1066/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1067/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1068/2000, total reward: -158.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1069/2000, total reward: -172.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1070/2000, total reward: -181.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1071/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1072/2000, total reward: -129.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1073/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1074/2000, total reward: -177.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1075/2000, total reward: -171.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1076/2000, total reward: -173.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1077/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1078/2000, total reward: -181.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1079/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1080/2000, total reward: -173.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1081/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1082/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1083/2000, total reward: -162.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1084/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1085/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1086/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1087/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1088/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1089/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1090/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1091/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1092/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1093/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1094/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1095/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1096/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1097/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1098/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1099/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1100/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1101/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1102/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1103/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1104/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1105/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1106/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1107/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1108/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1109/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1110/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1111/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1112/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1113/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1114/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1115/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1116/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1117/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1118/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1119/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1120/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1121/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1122/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1123/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1124/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1125/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1126/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1127/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1128/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1129/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1130/2000, total reward: -162.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1131/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1132/2000, total reward: -185.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1133/2000, total reward: -197.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1134/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1135/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1136/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1137/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1138/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1139/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1140/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1141/2000, total reward: -162.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1142/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1143/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1144/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1145/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1146/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1147/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1148/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1149/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1150/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1151/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1152/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1153/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1154/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1155/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1156/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1157/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1158/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1159/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1160/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1161/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1162/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1163/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1164/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1165/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1166/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1167/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1168/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1169/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1170/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1171/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1172/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1173/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1174/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1175/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1176/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1177/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1178/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1179/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1180/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1181/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1182/2000, total reward: -165.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1183/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1184/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1185/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1186/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1187/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1188/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1189/2000, total reward: -169.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1190/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1191/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1192/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1193/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1194/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1195/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1196/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1197/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1198/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1199/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1200/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1201/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1202/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1203/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1204/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1205/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1206/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1207/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1208/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1209/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1210/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1211/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1212/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1213/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1214/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1215/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1216/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1217/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1218/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1219/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1220/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1221/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1222/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1223/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1224/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1225/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1226/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1227/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1228/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1229/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1230/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1231/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1232/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1233/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1234/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1235/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1236/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1237/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1238/2000, total reward: -156.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1239/2000, total reward: -190.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1240/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1241/2000, total reward: -167.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1242/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1243/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1244/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1245/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1246/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1247/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1248/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1249/2000, total reward: -185.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1250/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1251/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1252/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1253/2000, total reward: -164.0,e: 0.047)\n",
      "episode: 1254/2000, total reward: -181.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1255/2000, total reward: -174.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1256/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1257/2000, total reward: -169.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1258/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1259/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1260/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1261/2000, total reward: -160.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1262/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1263/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1264/2000, total reward: -156.0,e: 0.047)\n",
      "episode: 1265/2000, total reward: -157.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1266/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1267/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1268/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1269/2000, total reward: -195.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1270/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1271/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1272/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1273/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1274/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1275/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1276/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1277/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1278/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1279/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1280/2000, total reward: -200.0,e: 0.047)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 1281/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1282/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1283/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1284/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1285/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1286/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1287/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1288/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1289/2000, total reward: -185.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1290/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1291/2000, total reward: -156.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1292/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1293/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1294/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1295/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1296/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1297/2000, total reward: -168.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1298/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1299/2000, total reward: -177.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1300/2000, total reward: -146.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1301/2000, total reward: -168.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1302/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1303/2000, total reward: -156.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1304/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1305/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1306/2000, total reward: -175.0,e: 0.047)\n",
      "episode: 1307/2000, total reward: -170.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1308/2000, total reward: -178.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1309/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1310/2000, total reward: -160.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1311/2000, total reward: -170.0,e: 0.047)\n",
      "episode: 1312/2000, total reward: -98.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1313/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1314/2000, total reward: -173.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1315/2000, total reward: -171.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1316/2000, total reward: -172.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1317/2000, total reward: -160.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1318/2000, total reward: -192.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1319/2000, total reward: -182.0,e: 0.047)\n",
      "episode: 1320/2000, total reward: -167.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1321/2000, total reward: -154.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1322/2000, total reward: -181.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1323/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1324/2000, total reward: -164.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1325/2000, total reward: -159.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1326/2000, total reward: -189.0,e: 0.047)\n",
      "episode: 1327/2000, total reward: -157.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1328/2000, total reward: -173.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1329/2000, total reward: -181.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1330/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1331/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1332/2000, total reward: -155.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1333/2000, total reward: -192.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1334/2000, total reward: -155.0,e: 0.047)\n",
      "episode: 1335/2000, total reward: -164.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1336/2000, total reward: -163.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1337/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1338/2000, total reward: -170.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1339/2000, total reward: -153.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1340/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1341/2000, total reward: -154.0,e: 0.047)\n",
      "episode: 1342/2000, total reward: -164.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1343/2000, total reward: -199.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1344/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1345/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1346/2000, total reward: -166.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1347/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1348/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1349/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1350/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1351/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1352/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1353/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1354/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1355/2000, total reward: -166.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1356/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1357/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1358/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1359/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1360/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1361/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1362/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1363/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1364/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1365/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1366/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1367/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1368/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1369/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1370/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1371/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1372/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1373/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1374/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1375/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1376/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1377/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1378/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1379/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1380/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1381/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1382/2000, total reward: -187.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1383/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1384/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1385/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1386/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1387/2000, total reward: -187.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1388/2000, total reward: -163.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1389/2000, total reward: -175.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1390/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1391/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1392/2000, total reward: -148.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1393/2000, total reward: -157.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1394/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1395/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1396/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1397/2000, total reward: -160.0,e: 0.047)\n",
      "updated target model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1398/2000, total reward: -172.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1399/2000, total reward: -152.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1400/2000, total reward: -163.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1401/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1402/2000, total reward: -191.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1403/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1404/2000, total reward: -167.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1405/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1406/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1407/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1408/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1409/2000, total reward: -198.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1410/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1411/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1412/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1413/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1414/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1415/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1416/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1417/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1418/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1419/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1420/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1421/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1422/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1423/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1424/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1425/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1426/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1427/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1428/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1429/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1430/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1431/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1432/2000, total reward: -160.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1433/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1434/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1435/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1436/2000, total reward: -190.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1437/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1438/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1439/2000, total reward: -161.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1440/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1441/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1442/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1443/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1444/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1445/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1446/2000, total reward: -173.0,e: 0.047)\n",
      "episode: 1447/2000, total reward: -177.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1448/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1449/2000, total reward: -167.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1450/2000, total reward: -191.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1451/2000, total reward: -196.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1452/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1453/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1454/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1455/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1456/2000, total reward: -188.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1457/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1458/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1459/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1460/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1461/2000, total reward: -190.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1462/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1463/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1464/2000, total reward: -159.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1465/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1466/2000, total reward: -173.0,e: 0.047)\n",
      "episode: 1467/2000, total reward: -179.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1468/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1469/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1470/2000, total reward: -185.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1471/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1472/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1473/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1474/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1475/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1476/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1477/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1478/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1479/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1480/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1481/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1482/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1483/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1484/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1485/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1486/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1487/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1488/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1489/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1490/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1491/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1492/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1493/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1494/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1495/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1496/2000, total reward: -162.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1497/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1498/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1499/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1500/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1501/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1502/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1503/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1504/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1505/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1506/2000, total reward: -194.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1507/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1508/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1509/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1510/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1511/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1512/2000, total reward: -200.0,e: 0.047)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 1513/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1514/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1515/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1516/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1517/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1518/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1519/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1520/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1521/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1522/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1523/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1524/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1525/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1526/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1527/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1528/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1529/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1530/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1531/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1532/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1533/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1534/2000, total reward: -167.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1535/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1536/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1537/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1538/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1539/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1540/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1541/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1542/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1543/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1544/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1545/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1546/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1547/2000, total reward: -166.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1548/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1549/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1550/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1551/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1552/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1553/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1554/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1555/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1556/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1557/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1558/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1559/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1560/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1561/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1562/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1563/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1564/2000, total reward: -182.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1565/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1566/2000, total reward: -146.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1567/2000, total reward: -146.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1568/2000, total reward: -171.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1569/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1570/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1571/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1572/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1573/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1574/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1575/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1576/2000, total reward: -156.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1577/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1578/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1579/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1580/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1581/2000, total reward: -153.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1582/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1583/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1584/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1585/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1586/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1587/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1588/2000, total reward: -176.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1589/2000, total reward: -91.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1590/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1591/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1592/2000, total reward: -177.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1593/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1594/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1595/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1596/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1597/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1598/2000, total reward: -151.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1599/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1600/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1601/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1602/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1603/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1604/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1605/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1606/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1607/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1608/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1609/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1610/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1611/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1612/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1613/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1614/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1615/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1616/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1617/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1618/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1619/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1620/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1621/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1622/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1623/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1624/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1625/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1626/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1627/2000, total reward: -200.0,e: 0.047)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 1628/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1629/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1630/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1631/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1632/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1633/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1634/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1635/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1636/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1637/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1638/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1639/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1640/2000, total reward: -170.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1641/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1642/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1643/2000, total reward: -156.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1644/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1645/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1646/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1647/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1648/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1649/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1650/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1651/2000, total reward: -192.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1652/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1653/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1654/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1655/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1656/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1657/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1658/2000, total reward: -191.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1659/2000, total reward: -160.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1660/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1661/2000, total reward: -186.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1662/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1663/2000, total reward: -136.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1664/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1665/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1666/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1667/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1668/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1669/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1670/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1671/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1672/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1673/2000, total reward: -163.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1674/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1675/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1676/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1677/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1678/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1679/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1680/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1681/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1682/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1683/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1684/2000, total reward: -179.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1685/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1686/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1687/2000, total reward: -178.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1688/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1689/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1690/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1691/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1692/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1693/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1694/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1695/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1696/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1697/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1698/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1699/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1700/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1701/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1702/2000, total reward: -172.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1703/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1704/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1705/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1706/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1707/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1708/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1709/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1710/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1711/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1712/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1713/2000, total reward: -182.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1714/2000, total reward: -171.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1715/2000, total reward: -151.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1716/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1717/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1718/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1719/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1720/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1721/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1722/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1723/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1724/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1725/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1726/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1727/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1728/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1729/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1730/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1731/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1732/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1733/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1734/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1735/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1736/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1737/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1738/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1739/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1740/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1741/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1742/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1743/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1744/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1745/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1746/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1747/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1748/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1749/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1750/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1751/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1752/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1753/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1754/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1755/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1756/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1757/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1758/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1759/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1760/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1761/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1762/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1763/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1764/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1765/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1766/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1767/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1768/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1769/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1770/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1771/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1772/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1773/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1774/2000, total reward: -153.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1775/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1776/2000, total reward: -187.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1777/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1778/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1779/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1780/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1781/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1782/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1783/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1784/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1785/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1786/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1787/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1788/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1789/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1790/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1791/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1792/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1793/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1794/2000, total reward: -164.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1795/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1796/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1797/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1798/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1799/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1800/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1801/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1802/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1803/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1804/2000, total reward: -187.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1805/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1806/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1807/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1808/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1809/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1810/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1811/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1812/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1813/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1814/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1815/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1816/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1817/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1818/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1819/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1820/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1821/2000, total reward: -183.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1822/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1823/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1824/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1825/2000, total reward: -168.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1826/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1827/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1828/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1829/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1830/2000, total reward: -190.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1831/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1832/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1833/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1834/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1835/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1836/2000, total reward: -161.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1837/2000, total reward: -191.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1838/2000, total reward: -165.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1839/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1840/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1841/2000, total reward: -167.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1842/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1843/2000, total reward: -119.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1844/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1845/2000, total reward: -196.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1846/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1847/2000, total reward: -176.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1848/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1849/2000, total reward: -175.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1850/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1851/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1852/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1853/2000, total reward: -174.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1854/2000, total reward: -197.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1855/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1856/2000, total reward: -174.0,e: 0.047)\n",
      "updated target model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1857/2000, total reward: -161.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1858/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1859/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1860/2000, total reward: -193.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1861/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1862/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1863/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1864/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1865/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1866/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1867/2000, total reward: -162.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1868/2000, total reward: -159.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1869/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1870/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1871/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1872/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1873/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1874/2000, total reward: -158.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1875/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1876/2000, total reward: -165.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1877/2000, total reward: -150.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1878/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1879/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1880/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1881/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1882/2000, total reward: -146.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1883/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1884/2000, total reward: -181.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1885/2000, total reward: -158.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1886/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1887/2000, total reward: -145.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1888/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1889/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1890/2000, total reward: -160.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1891/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1892/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1893/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1894/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1895/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1896/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1897/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1898/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1899/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1900/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1901/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1902/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1903/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1904/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1905/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1906/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1907/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1908/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1909/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1910/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1911/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1912/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1913/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1914/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1915/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1916/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1917/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1918/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1919/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1920/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1921/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1922/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1923/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1924/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1925/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1926/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1927/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1928/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1929/2000, total reward: -138.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1930/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1931/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1932/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1933/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1934/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1935/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1936/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1937/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1938/2000, total reward: -166.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1939/2000, total reward: -172.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1940/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1941/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1942/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1943/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1944/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1945/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1946/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1947/2000, total reward: -162.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1948/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1949/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1950/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1951/2000, total reward: -163.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1952/2000, total reward: -169.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1953/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1954/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1955/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1956/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1957/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1958/2000, total reward: -163.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1959/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1960/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1961/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1962/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1963/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1964/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1965/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1966/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1967/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 1968/2000, total reward: -167.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1969/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1970/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1971/2000, total reward: -154.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1972/2000, total reward: -200.0,e: 0.047)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 1973/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1974/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1975/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1976/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1977/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1978/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1979/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1980/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1981/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1982/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1983/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1984/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1985/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1986/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1987/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1988/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1989/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1990/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1991/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1992/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1993/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1994/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1995/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1996/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1997/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1998/2000, total reward: -200.0,e: 0.047)\n",
      "updated target model\n",
      "episode: 1999/2000, total reward: -200.0,e: 0.047)\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "done = False\n",
    "rewards = []\n",
    "losses=[]\n",
    "update_target_step = 200\n",
    "step_counter = 0\n",
    "losses = []\n",
    "steps = 0\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for time in range(200):\n",
    "        \n",
    "\n",
    "        action = act(state)\n",
    "    \n",
    "\n",
    "        next_state, reward, done,_ = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        store(state,action,reward,next_state,done)\n",
    "        \n",
    "        if done:\n",
    "            print('episode: {}/{}, total reward: {},e: {:.2})'.format(e,n_episodes,total_reward,eps))\n",
    "            rewards.append(total_reward)\n",
    "            break\n",
    "        \n",
    "        if len(memory)>= batch_size:\n",
    "            loss = update(memory,batch_size)\n",
    "            \n",
    "       \n",
    "\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        step_counter += 1\n",
    "        if (step_counter > update_target_step):\n",
    "                target_model.load_state_dict(dqn.state_dict())\n",
    "                step_counter = 0\n",
    "                print('updated target model')\n",
    "\n",
    "    if eps > epsi_low:            \n",
    "        eps = eps*decay\n",
    "            \n",
    "    losses.append(loss)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RND-DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,n_hid):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_hid = n_hid\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.in_dim,self.n_hid)\n",
    "        self.fc2 = nn.Linear(self.n_hid,self.out_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,n_hid):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_hid = n_hid\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_dim,n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid,n_hid)\n",
    "        self.fc3 = nn.Linear(n_hid,out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RND():\n",
    "    def __init__(self,in_dim,out_dim,n_hid):\n",
    "        self.target = NN(in_dim,out_dim,n_hid)\n",
    "        self.model = NN(in_dim,out_dim,n_hid)\n",
    "        self.optimizer = Adam(self.model.parameters(),lr = 0.001)\n",
    "        \n",
    "    def get_reward(self,x):\n",
    "        y_true = self.target(x).detach()\n",
    "        y_pred = self.model(x)\n",
    "        reward = torch.pow(y_pred - y_true,2).sum()\n",
    "        return reward\n",
    "    \n",
    "    def update(self,Ri):\n",
    "        Ri.sum().backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = QNet(2,3,32)\n",
    "rnd = RND(2,128,124)\n",
    "target_model = copy.deepcopy(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1.0\n",
    "batch_size = 32\n",
    "\n",
    "gamma = 0.95\n",
    "eps_min = 0.01\n",
    "eps_decay = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.SmoothL1Loss()\n",
    "opt = Adam(dqn.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(s,a,r,s_next,done):\n",
    "    memory.append((s,a,r,s_next,done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(state):\n",
    "        if (np.random.random() < eps):\n",
    "                action = np.random.choice([0,1,2])\n",
    "        else:\n",
    "                action = torch.argmax(dqn(torch.FloatTensor(state).unsqueeze(0)),dim=1).detach().numpy().item()\n",
    "                \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(memory,batch_size):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        samples = random.sample(memory,batch_size)\n",
    "        \n",
    "        \n",
    "        s,a,r,s_next,d = (zip(*samples))\n",
    "        \n",
    "        s = torch.FloatTensor(s)\n",
    "        a = torch.LongTensor(a)\n",
    "        r = torch.FloatTensor(r)\n",
    "        s_next = torch.FloatTensor(s_next)\n",
    "        d = torch.FloatTensor(d)\n",
    "\n",
    "       \n",
    "        Ri = rnd.get_reward(s)\n",
    "        rnd.update(Ri)\n",
    "        \n",
    "        target_q = (r + gamma*target_model(s_next).max(dim=1)[0].detach()*(1 - d)).unsqueeze(1)\n",
    "        policy_q = dqn(s).gather(1,a.unsqueeze(1))\n",
    "        L = F.smooth_l1_loss(policy_q,target_q)\n",
    "        L.backward()\n",
    "        opt.step()\n",
    "        return L.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/2000, total reward: -200.0,e: 1.0)\n",
      "episode: 1/2000, total reward: -200.0,e: 0.99)\n",
      "updated target model\n",
      "episode: 2/2000, total reward: -200.0,e: 0.98)\n",
      "episode: 3/2000, total reward: -200.0,e: 0.97)\n",
      "episode: 4/2000, total reward: -200.0,e: 0.96)\n",
      "updated target model\n",
      "episode: 5/2000, total reward: -200.0,e: 0.95)\n",
      "episode: 6/2000, total reward: -200.0,e: 0.94)\n",
      "updated target model\n",
      "episode: 7/2000, total reward: -200.0,e: 0.93)\n",
      "episode: 8/2000, total reward: -200.0,e: 0.92)\n",
      "episode: 9/2000, total reward: -200.0,e: 0.91)\n",
      "updated target model\n",
      "episode: 10/2000, total reward: -200.0,e: 0.9)\n",
      "episode: 11/2000, total reward: -200.0,e: 0.9)\n",
      "updated target model\n",
      "episode: 12/2000, total reward: -200.0,e: 0.89)\n",
      "episode: 13/2000, total reward: -200.0,e: 0.88)\n",
      "episode: 14/2000, total reward: -200.0,e: 0.87)\n",
      "updated target model\n",
      "episode: 15/2000, total reward: -200.0,e: 0.86)\n",
      "episode: 16/2000, total reward: -200.0,e: 0.85)\n",
      "updated target model\n",
      "episode: 17/2000, total reward: -200.0,e: 0.84)\n",
      "episode: 18/2000, total reward: -200.0,e: 0.83)\n",
      "episode: 19/2000, total reward: -200.0,e: 0.83)\n",
      "updated target model\n",
      "episode: 20/2000, total reward: -200.0,e: 0.82)\n",
      "episode: 21/2000, total reward: -200.0,e: 0.81)\n",
      "updated target model\n",
      "episode: 22/2000, total reward: -200.0,e: 0.8)\n",
      "episode: 23/2000, total reward: -200.0,e: 0.79)\n",
      "episode: 24/2000, total reward: -200.0,e: 0.79)\n",
      "updated target model\n",
      "episode: 25/2000, total reward: -200.0,e: 0.78)\n",
      "episode: 26/2000, total reward: -200.0,e: 0.77)\n",
      "updated target model\n",
      "episode: 27/2000, total reward: -200.0,e: 0.76)\n",
      "episode: 28/2000, total reward: -200.0,e: 0.75)\n",
      "episode: 29/2000, total reward: -200.0,e: 0.75)\n",
      "updated target model\n",
      "episode: 30/2000, total reward: -200.0,e: 0.74)\n",
      "episode: 31/2000, total reward: -200.0,e: 0.73)\n",
      "updated target model\n",
      "episode: 32/2000, total reward: -200.0,e: 0.72)\n",
      "episode: 33/2000, total reward: -200.0,e: 0.72)\n",
      "episode: 34/2000, total reward: -200.0,e: 0.71)\n",
      "updated target model\n",
      "episode: 35/2000, total reward: -200.0,e: 0.7)\n",
      "episode: 36/2000, total reward: -200.0,e: 0.7)\n",
      "updated target model\n",
      "episode: 37/2000, total reward: -200.0,e: 0.69)\n",
      "episode: 38/2000, total reward: -200.0,e: 0.68)\n",
      "episode: 39/2000, total reward: -200.0,e: 0.68)\n",
      "updated target model\n",
      "episode: 40/2000, total reward: -200.0,e: 0.67)\n",
      "episode: 41/2000, total reward: -200.0,e: 0.66)\n",
      "updated target model\n",
      "episode: 42/2000, total reward: -200.0,e: 0.66)\n",
      "episode: 43/2000, total reward: -200.0,e: 0.65)\n",
      "episode: 44/2000, total reward: -200.0,e: 0.64)\n",
      "updated target model\n",
      "episode: 45/2000, total reward: -200.0,e: 0.64)\n",
      "episode: 46/2000, total reward: -200.0,e: 0.63)\n",
      "updated target model\n",
      "episode: 47/2000, total reward: -200.0,e: 0.62)\n",
      "episode: 48/2000, total reward: -200.0,e: 0.62)\n",
      "episode: 49/2000, total reward: -200.0,e: 0.61)\n",
      "updated target model\n",
      "episode: 50/2000, total reward: -200.0,e: 0.61)\n",
      "episode: 51/2000, total reward: -200.0,e: 0.6)\n",
      "updated target model\n",
      "episode: 52/2000, total reward: -200.0,e: 0.59)\n",
      "episode: 53/2000, total reward: -200.0,e: 0.59)\n",
      "episode: 54/2000, total reward: -200.0,e: 0.58)\n",
      "updated target model\n",
      "episode: 55/2000, total reward: -200.0,e: 0.58)\n",
      "episode: 56/2000, total reward: -200.0,e: 0.57)\n",
      "updated target model\n",
      "episode: 57/2000, total reward: -200.0,e: 0.56)\n",
      "episode: 58/2000, total reward: -200.0,e: 0.56)\n",
      "episode: 59/2000, total reward: -200.0,e: 0.55)\n",
      "updated target model\n",
      "episode: 60/2000, total reward: -200.0,e: 0.55)\n",
      "episode: 61/2000, total reward: -200.0,e: 0.54)\n",
      "updated target model\n",
      "episode: 62/2000, total reward: -200.0,e: 0.54)\n",
      "episode: 63/2000, total reward: -200.0,e: 0.53)\n",
      "episode: 64/2000, total reward: -200.0,e: 0.53)\n",
      "updated target model\n",
      "episode: 65/2000, total reward: -200.0,e: 0.52)\n",
      "episode: 66/2000, total reward: -200.0,e: 0.52)\n",
      "updated target model\n",
      "episode: 67/2000, total reward: -200.0,e: 0.51)\n",
      "episode: 68/2000, total reward: -200.0,e: 0.5)\n",
      "episode: 69/2000, total reward: -200.0,e: 0.5)\n",
      "updated target model\n",
      "episode: 70/2000, total reward: -200.0,e: 0.49)\n",
      "episode: 71/2000, total reward: -200.0,e: 0.49)\n",
      "episode: 72/2000, total reward: -200.0,e: 0.48)\n",
      "updated target model\n",
      "episode: 73/2000, total reward: -200.0,e: 0.48)\n",
      "episode: 74/2000, total reward: -200.0,e: 0.48)\n",
      "updated target model\n",
      "episode: 75/2000, total reward: -200.0,e: 0.47)\n",
      "episode: 76/2000, total reward: -200.0,e: 0.47)\n",
      "episode: 77/2000, total reward: -200.0,e: 0.46)\n",
      "updated target model\n",
      "episode: 78/2000, total reward: -200.0,e: 0.46)\n",
      "episode: 79/2000, total reward: -200.0,e: 0.45)\n",
      "updated target model\n",
      "episode: 80/2000, total reward: -200.0,e: 0.45)\n",
      "episode: 81/2000, total reward: -200.0,e: 0.44)\n",
      "episode: 82/2000, total reward: -200.0,e: 0.44)\n",
      "updated target model\n",
      "episode: 83/2000, total reward: -200.0,e: 0.43)\n",
      "episode: 84/2000, total reward: -200.0,e: 0.43)\n",
      "updated target model\n",
      "episode: 85/2000, total reward: -200.0,e: 0.43)\n",
      "episode: 86/2000, total reward: -200.0,e: 0.42)\n",
      "episode: 87/2000, total reward: -200.0,e: 0.42)\n",
      "updated target model\n",
      "episode: 88/2000, total reward: -200.0,e: 0.41)\n",
      "episode: 89/2000, total reward: -200.0,e: 0.41)\n",
      "updated target model\n",
      "episode: 90/2000, total reward: -200.0,e: 0.4)\n",
      "episode: 91/2000, total reward: -200.0,e: 0.4)\n",
      "episode: 92/2000, total reward: -200.0,e: 0.4)\n",
      "updated target model\n",
      "episode: 93/2000, total reward: -200.0,e: 0.39)\n",
      "episode: 94/2000, total reward: -200.0,e: 0.39)\n",
      "updated target model\n",
      "episode: 95/2000, total reward: -200.0,e: 0.38)\n",
      "episode: 96/2000, total reward: -200.0,e: 0.38)\n",
      "episode: 97/2000, total reward: -200.0,e: 0.38)\n",
      "updated target model\n",
      "episode: 98/2000, total reward: -200.0,e: 0.37)\n",
      "episode: 99/2000, total reward: -200.0,e: 0.37)\n",
      "updated target model\n",
      "episode: 100/2000, total reward: -200.0,e: 0.37)\n",
      "episode: 101/2000, total reward: -200.0,e: 0.36)\n",
      "episode: 102/2000, total reward: -200.0,e: 0.36)\n",
      "updated target model\n",
      "episode: 103/2000, total reward: -200.0,e: 0.36)\n",
      "episode: 104/2000, total reward: -200.0,e: 0.35)\n",
      "updated target model\n",
      "episode: 105/2000, total reward: -200.0,e: 0.35)\n",
      "episode: 106/2000, total reward: -200.0,e: 0.34)\n",
      "episode: 107/2000, total reward: -200.0,e: 0.34)\n",
      "updated target model\n",
      "episode: 108/2000, total reward: -200.0,e: 0.34)\n",
      "episode: 109/2000, total reward: -200.0,e: 0.33)\n",
      "updated target model\n",
      "episode: 110/2000, total reward: -200.0,e: 0.33)\n",
      "episode: 111/2000, total reward: -200.0,e: 0.33)\n",
      "episode: 112/2000, total reward: -200.0,e: 0.32)\n",
      "updated target model\n",
      "episode: 113/2000, total reward: -200.0,e: 0.32)\n",
      "episode: 114/2000, total reward: -200.0,e: 0.32)\n",
      "updated target model\n",
      "episode: 115/2000, total reward: -200.0,e: 0.31)\n",
      "episode: 116/2000, total reward: -200.0,e: 0.31)\n",
      "episode: 117/2000, total reward: -200.0,e: 0.31)\n",
      "updated target model\n",
      "episode: 118/2000, total reward: -200.0,e: 0.31)\n",
      "episode: 119/2000, total reward: -200.0,e: 0.3)\n",
      "updated target model\n",
      "episode: 120/2000, total reward: -200.0,e: 0.3)\n",
      "episode: 121/2000, total reward: -200.0,e: 0.3)\n",
      "episode: 122/2000, total reward: -200.0,e: 0.29)\n",
      "updated target model\n",
      "episode: 123/2000, total reward: -200.0,e: 0.29)\n",
      "episode: 124/2000, total reward: -200.0,e: 0.29)\n",
      "updated target model\n",
      "episode: 125/2000, total reward: -200.0,e: 0.28)\n",
      "episode: 126/2000, total reward: -200.0,e: 0.28)\n",
      "episode: 127/2000, total reward: -200.0,e: 0.28)\n",
      "updated target model\n",
      "episode: 128/2000, total reward: -200.0,e: 0.28)\n",
      "episode: 129/2000, total reward: -200.0,e: 0.27)\n",
      "updated target model\n",
      "episode: 130/2000, total reward: -200.0,e: 0.27)\n",
      "episode: 131/2000, total reward: -200.0,e: 0.27)\n",
      "episode: 132/2000, total reward: -200.0,e: 0.27)\n",
      "updated target model\n",
      "episode: 133/2000, total reward: -200.0,e: 0.26)\n",
      "episode: 134/2000, total reward: -200.0,e: 0.26)\n",
      "updated target model\n",
      "episode: 135/2000, total reward: -200.0,e: 0.26)\n",
      "episode: 136/2000, total reward: -200.0,e: 0.25)\n",
      "episode: 137/2000, total reward: -200.0,e: 0.25)\n",
      "updated target model\n",
      "episode: 138/2000, total reward: -200.0,e: 0.25)\n",
      "episode: 139/2000, total reward: -200.0,e: 0.25)\n",
      "updated target model\n",
      "episode: 140/2000, total reward: -200.0,e: 0.24)\n",
      "episode: 141/2000, total reward: -200.0,e: 0.24)\n",
      "episode: 142/2000, total reward: -200.0,e: 0.24)\n",
      "updated target model\n",
      "episode: 143/2000, total reward: -200.0,e: 0.24)\n",
      "episode: 144/2000, total reward: -200.0,e: 0.24)\n",
      "episode: 145/2000, total reward: -200.0,e: 0.23)\n",
      "updated target model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 146/2000, total reward: -200.0,e: 0.23)\n",
      "episode: 147/2000, total reward: -200.0,e: 0.23)\n",
      "updated target model\n",
      "episode: 148/2000, total reward: -200.0,e: 0.23)\n",
      "episode: 149/2000, total reward: -200.0,e: 0.22)\n",
      "episode: 150/2000, total reward: -200.0,e: 0.22)\n",
      "updated target model\n",
      "episode: 151/2000, total reward: -200.0,e: 0.22)\n",
      "episode: 152/2000, total reward: -200.0,e: 0.22)\n",
      "updated target model\n",
      "episode: 153/2000, total reward: -200.0,e: 0.21)\n",
      "episode: 154/2000, total reward: -200.0,e: 0.21)\n",
      "episode: 155/2000, total reward: -200.0,e: 0.21)\n",
      "updated target model\n",
      "episode: 156/2000, total reward: -200.0,e: 0.21)\n",
      "episode: 157/2000, total reward: -200.0,e: 0.21)\n",
      "updated target model\n",
      "episode: 158/2000, total reward: -200.0,e: 0.2)\n",
      "episode: 159/2000, total reward: -200.0,e: 0.2)\n",
      "episode: 160/2000, total reward: -200.0,e: 0.2)\n",
      "updated target model\n",
      "episode: 161/2000, total reward: -200.0,e: 0.2)\n",
      "episode: 162/2000, total reward: -200.0,e: 0.2)\n",
      "updated target model\n",
      "episode: 163/2000, total reward: -200.0,e: 0.19)\n",
      "episode: 164/2000, total reward: -200.0,e: 0.19)\n",
      "episode: 165/2000, total reward: -200.0,e: 0.19)\n",
      "updated target model\n",
      "episode: 166/2000, total reward: -200.0,e: 0.19)\n",
      "episode: 167/2000, total reward: -200.0,e: 0.19)\n",
      "updated target model\n",
      "episode: 168/2000, total reward: -200.0,e: 0.18)\n",
      "episode: 169/2000, total reward: -200.0,e: 0.18)\n",
      "episode: 170/2000, total reward: -200.0,e: 0.18)\n",
      "updated target model\n",
      "episode: 171/2000, total reward: -200.0,e: 0.18)\n",
      "episode: 172/2000, total reward: -200.0,e: 0.18)\n",
      "updated target model\n",
      "episode: 173/2000, total reward: -200.0,e: 0.18)\n",
      "episode: 174/2000, total reward: -200.0,e: 0.17)\n",
      "episode: 175/2000, total reward: -200.0,e: 0.17)\n",
      "updated target model\n",
      "episode: 176/2000, total reward: -200.0,e: 0.17)\n",
      "episode: 177/2000, total reward: -200.0,e: 0.17)\n",
      "updated target model\n",
      "episode: 178/2000, total reward: -200.0,e: 0.17)\n",
      "episode: 179/2000, total reward: -200.0,e: 0.17)\n",
      "episode: 180/2000, total reward: -200.0,e: 0.16)\n",
      "updated target model\n",
      "episode: 181/2000, total reward: -200.0,e: 0.16)\n",
      "episode: 182/2000, total reward: -200.0,e: 0.16)\n",
      "updated target model\n",
      "episode: 183/2000, total reward: -200.0,e: 0.16)\n",
      "episode: 184/2000, total reward: -200.0,e: 0.16)\n",
      "episode: 185/2000, total reward: -200.0,e: 0.16)\n",
      "updated target model\n",
      "episode: 186/2000, total reward: -200.0,e: 0.15)\n",
      "episode: 187/2000, total reward: -200.0,e: 0.15)\n",
      "updated target model\n",
      "episode: 188/2000, total reward: -200.0,e: 0.15)\n",
      "episode: 189/2000, total reward: -200.0,e: 0.15)\n",
      "episode: 190/2000, total reward: -200.0,e: 0.15)\n",
      "updated target model\n",
      "episode: 191/2000, total reward: -200.0,e: 0.15)\n",
      "episode: 192/2000, total reward: -200.0,e: 0.15)\n",
      "updated target model\n",
      "episode: 193/2000, total reward: -200.0,e: 0.14)\n",
      "episode: 194/2000, total reward: -200.0,e: 0.14)\n",
      "episode: 195/2000, total reward: -200.0,e: 0.14)\n",
      "updated target model\n",
      "episode: 196/2000, total reward: -200.0,e: 0.14)\n",
      "episode: 197/2000, total reward: -200.0,e: 0.14)\n",
      "updated target model\n",
      "episode: 198/2000, total reward: -200.0,e: 0.14)\n",
      "episode: 199/2000, total reward: -200.0,e: 0.14)\n",
      "episode: 200/2000, total reward: -200.0,e: 0.13)\n",
      "updated target model\n",
      "episode: 201/2000, total reward: -200.0,e: 0.13)\n",
      "episode: 202/2000, total reward: -200.0,e: 0.13)\n",
      "updated target model\n",
      "episode: 203/2000, total reward: -200.0,e: 0.13)\n",
      "episode: 204/2000, total reward: -200.0,e: 0.13)\n",
      "episode: 205/2000, total reward: -200.0,e: 0.13)\n",
      "updated target model\n",
      "episode: 206/2000, total reward: -200.0,e: 0.13)\n",
      "episode: 207/2000, total reward: -200.0,e: 0.12)\n",
      "updated target model\n",
      "episode: 208/2000, total reward: -200.0,e: 0.12)\n",
      "episode: 209/2000, total reward: -200.0,e: 0.12)\n",
      "episode: 210/2000, total reward: -200.0,e: 0.12)\n",
      "updated target model\n",
      "episode: 211/2000, total reward: -200.0,e: 0.12)\n",
      "episode: 212/2000, total reward: -200.0,e: 0.12)\n",
      "updated target model\n",
      "episode: 213/2000, total reward: -200.0,e: 0.12)\n",
      "episode: 214/2000, total reward: -200.0,e: 0.12)\n",
      "episode: 215/2000, total reward: -200.0,e: 0.12)\n",
      "updated target model\n",
      "episode: 216/2000, total reward: -200.0,e: 0.11)\n",
      "episode: 217/2000, total reward: -200.0,e: 0.11)\n",
      "episode: 218/2000, total reward: -200.0,e: 0.11)\n",
      "updated target model\n",
      "episode: 219/2000, total reward: -200.0,e: 0.11)\n",
      "episode: 220/2000, total reward: -200.0,e: 0.11)\n",
      "updated target model\n",
      "episode: 221/2000, total reward: -200.0,e: 0.11)\n",
      "episode: 222/2000, total reward: -200.0,e: 0.11)\n",
      "episode: 223/2000, total reward: -200.0,e: 0.11)\n",
      "updated target model\n",
      "episode: 224/2000, total reward: -200.0,e: 0.11)\n",
      "episode: 225/2000, total reward: -200.0,e: 0.1)\n",
      "updated target model\n",
      "episode: 226/2000, total reward: -200.0,e: 0.1)\n",
      "episode: 227/2000, total reward: -200.0,e: 0.1)\n",
      "episode: 228/2000, total reward: -200.0,e: 0.1)\n",
      "updated target model\n",
      "episode: 229/2000, total reward: -200.0,e: 0.1)\n",
      "episode: 230/2000, total reward: -200.0,e: 0.099)\n",
      "updated target model\n",
      "episode: 231/2000, total reward: -200.0,e: 0.098)\n",
      "episode: 232/2000, total reward: -200.0,e: 0.097)\n",
      "episode: 233/2000, total reward: -200.0,e: 0.096)\n",
      "updated target model\n",
      "episode: 234/2000, total reward: -200.0,e: 0.095)\n",
      "episode: 235/2000, total reward: -200.0,e: 0.094)\n",
      "updated target model\n",
      "episode: 236/2000, total reward: -200.0,e: 0.093)\n",
      "episode: 237/2000, total reward: -200.0,e: 0.092)\n",
      "episode: 238/2000, total reward: -200.0,e: 0.091)\n",
      "updated target model\n",
      "episode: 239/2000, total reward: -200.0,e: 0.091)\n",
      "episode: 240/2000, total reward: -200.0,e: 0.09)\n",
      "updated target model\n",
      "episode: 241/2000, total reward: -200.0,e: 0.089)\n",
      "episode: 242/2000, total reward: -200.0,e: 0.088)\n",
      "episode: 243/2000, total reward: -200.0,e: 0.087)\n",
      "updated target model\n",
      "episode: 244/2000, total reward: -200.0,e: 0.086)\n",
      "episode: 245/2000, total reward: -200.0,e: 0.085)\n",
      "updated target model\n",
      "episode: 246/2000, total reward: -200.0,e: 0.084)\n",
      "episode: 247/2000, total reward: -200.0,e: 0.084)\n",
      "episode: 248/2000, total reward: -200.0,e: 0.083)\n",
      "updated target model\n",
      "episode: 249/2000, total reward: -200.0,e: 0.082)\n",
      "episode: 250/2000, total reward: -200.0,e: 0.081)\n",
      "updated target model\n",
      "episode: 251/2000, total reward: -200.0,e: 0.08)\n",
      "episode: 252/2000, total reward: -200.0,e: 0.079)\n",
      "episode: 253/2000, total reward: -200.0,e: 0.079)\n",
      "updated target model\n",
      "episode: 254/2000, total reward: -200.0,e: 0.078)\n",
      "episode: 255/2000, total reward: -200.0,e: 0.077)\n",
      "updated target model\n",
      "episode: 256/2000, total reward: -200.0,e: 0.076)\n",
      "episode: 257/2000, total reward: -200.0,e: 0.076)\n",
      "episode: 258/2000, total reward: -200.0,e: 0.075)\n",
      "updated target model\n",
      "episode: 259/2000, total reward: -200.0,e: 0.074)\n",
      "episode: 260/2000, total reward: -200.0,e: 0.073)\n",
      "updated target model\n",
      "episode: 261/2000, total reward: -200.0,e: 0.073)\n",
      "episode: 262/2000, total reward: -200.0,e: 0.072)\n",
      "episode: 263/2000, total reward: -200.0,e: 0.071)\n",
      "updated target model\n",
      "episode: 264/2000, total reward: -200.0,e: 0.07)\n",
      "episode: 265/2000, total reward: -200.0,e: 0.07)\n",
      "updated target model\n",
      "episode: 266/2000, total reward: -200.0,e: 0.069)\n",
      "episode: 267/2000, total reward: -200.0,e: 0.068)\n",
      "episode: 268/2000, total reward: -200.0,e: 0.068)\n",
      "updated target model\n",
      "episode: 269/2000, total reward: -200.0,e: 0.067)\n",
      "episode: 270/2000, total reward: -200.0,e: 0.066)\n",
      "updated target model\n",
      "episode: 271/2000, total reward: -200.0,e: 0.066)\n",
      "episode: 272/2000, total reward: -200.0,e: 0.065)\n",
      "episode: 273/2000, total reward: -200.0,e: 0.064)\n",
      "updated target model\n",
      "episode: 274/2000, total reward: -200.0,e: 0.064)\n",
      "episode: 275/2000, total reward: -200.0,e: 0.063)\n",
      "updated target model\n",
      "episode: 276/2000, total reward: -200.0,e: 0.062)\n",
      "episode: 277/2000, total reward: -200.0,e: 0.062)\n",
      "episode: 278/2000, total reward: -200.0,e: 0.061)\n",
      "updated target model\n",
      "episode: 279/2000, total reward: -200.0,e: 0.061)\n",
      "episode: 280/2000, total reward: -200.0,e: 0.06)\n",
      "updated target model\n",
      "episode: 281/2000, total reward: -200.0,e: 0.059)\n",
      "episode: 282/2000, total reward: -200.0,e: 0.059)\n",
      "episode: 283/2000, total reward: -200.0,e: 0.058)\n",
      "updated target model\n",
      "episode: 284/2000, total reward: -200.0,e: 0.058)\n",
      "episode: 285/2000, total reward: -200.0,e: 0.057)\n",
      "episode: 286/2000, total reward: -200.0,e: 0.056)\n",
      "updated target model\n",
      "episode: 287/2000, total reward: -200.0,e: 0.056)\n",
      "episode: 288/2000, total reward: -200.0,e: 0.055)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 289/2000, total reward: -200.0,e: 0.055)\n",
      "episode: 290/2000, total reward: -200.0,e: 0.054)\n",
      "episode: 291/2000, total reward: -200.0,e: 0.054)\n",
      "updated target model\n",
      "episode: 292/2000, total reward: -200.0,e: 0.053)\n",
      "episode: 293/2000, total reward: -200.0,e: 0.053)\n",
      "updated target model\n",
      "episode: 294/2000, total reward: -200.0,e: 0.052)\n",
      "episode: 295/2000, total reward: -200.0,e: 0.052)\n",
      "episode: 296/2000, total reward: -200.0,e: 0.051)\n",
      "updated target model\n",
      "episode: 297/2000, total reward: -200.0,e: 0.051)\n",
      "episode: 298/2000, total reward: -200.0,e: 0.05)\n",
      "updated target model\n",
      "episode: 299/2000, total reward: -200.0,e: 0.05)\n",
      "episode: 300/2000, total reward: -200.0,e: 0.049)\n",
      "episode: 301/2000, total reward: -200.0,e: 0.049)\n",
      "updated target model\n",
      "episode: 302/2000, total reward: -200.0,e: 0.048)\n",
      "episode: 303/2000, total reward: -200.0,e: 0.048)\n",
      "updated target model\n",
      "episode: 304/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 305/2000, total reward: -200.0,e: 0.047)\n",
      "episode: 306/2000, total reward: -200.0,e: 0.046)\n",
      "updated target model\n",
      "episode: 307/2000, total reward: -200.0,e: 0.046)\n",
      "episode: 308/2000, total reward: -200.0,e: 0.045)\n",
      "updated target model\n",
      "episode: 309/2000, total reward: -200.0,e: 0.045)\n",
      "episode: 310/2000, total reward: -200.0,e: 0.044)\n",
      "episode: 311/2000, total reward: -200.0,e: 0.044)\n",
      "updated target model\n",
      "episode: 312/2000, total reward: -200.0,e: 0.043)\n",
      "episode: 313/2000, total reward: -200.0,e: 0.043)\n",
      "updated target model\n",
      "episode: 314/2000, total reward: -200.0,e: 0.043)\n",
      "episode: 315/2000, total reward: -200.0,e: 0.042)\n",
      "episode: 316/2000, total reward: -200.0,e: 0.042)\n",
      "updated target model\n",
      "episode: 317/2000, total reward: -200.0,e: 0.041)\n",
      "episode: 318/2000, total reward: -200.0,e: 0.041)\n",
      "updated target model\n",
      "episode: 319/2000, total reward: -200.0,e: 0.041)\n",
      "episode: 320/2000, total reward: -200.0,e: 0.04)\n",
      "episode: 321/2000, total reward: -200.0,e: 0.04)\n",
      "updated target model\n",
      "episode: 322/2000, total reward: -200.0,e: 0.039)\n",
      "episode: 323/2000, total reward: -200.0,e: 0.039)\n",
      "updated target model\n",
      "episode: 324/2000, total reward: -200.0,e: 0.039)\n",
      "episode: 325/2000, total reward: -200.0,e: 0.038)\n",
      "episode: 326/2000, total reward: -200.0,e: 0.038)\n",
      "updated target model\n",
      "episode: 327/2000, total reward: -200.0,e: 0.037)\n",
      "episode: 328/2000, total reward: -200.0,e: 0.037)\n",
      "updated target model\n",
      "episode: 329/2000, total reward: -200.0,e: 0.037)\n",
      "episode: 330/2000, total reward: -200.0,e: 0.036)\n",
      "episode: 331/2000, total reward: -200.0,e: 0.036)\n",
      "updated target model\n",
      "episode: 332/2000, total reward: -200.0,e: 0.036)\n",
      "episode: 333/2000, total reward: -200.0,e: 0.035)\n",
      "updated target model\n",
      "episode: 334/2000, total reward: -200.0,e: 0.035)\n",
      "episode: 335/2000, total reward: -200.0,e: 0.034)\n",
      "episode: 336/2000, total reward: -200.0,e: 0.034)\n",
      "updated target model\n",
      "episode: 337/2000, total reward: -200.0,e: 0.034)\n",
      "episode: 338/2000, total reward: -200.0,e: 0.033)\n",
      "updated target model\n",
      "episode: 339/2000, total reward: -200.0,e: 0.033)\n",
      "episode: 340/2000, total reward: -200.0,e: 0.033)\n",
      "episode: 341/2000, total reward: -200.0,e: 0.032)\n",
      "updated target model\n",
      "episode: 342/2000, total reward: -200.0,e: 0.032)\n",
      "episode: 343/2000, total reward: -200.0,e: 0.032)\n",
      "updated target model\n",
      "episode: 344/2000, total reward: -200.0,e: 0.032)\n",
      "episode: 345/2000, total reward: -200.0,e: 0.031)\n",
      "episode: 346/2000, total reward: -200.0,e: 0.031)\n",
      "updated target model\n",
      "episode: 347/2000, total reward: -200.0,e: 0.031)\n",
      "episode: 348/2000, total reward: -200.0,e: 0.03)\n",
      "updated target model\n",
      "episode: 349/2000, total reward: -200.0,e: 0.03)\n",
      "episode: 350/2000, total reward: -200.0,e: 0.03)\n",
      "episode: 351/2000, total reward: -200.0,e: 0.029)\n",
      "updated target model\n",
      "episode: 352/2000, total reward: -200.0,e: 0.029)\n",
      "episode: 353/2000, total reward: -200.0,e: 0.029)\n",
      "updated target model\n",
      "episode: 354/2000, total reward: -200.0,e: 0.029)\n",
      "episode: 355/2000, total reward: -200.0,e: 0.028)\n",
      "episode: 356/2000, total reward: -200.0,e: 0.028)\n",
      "updated target model\n",
      "episode: 357/2000, total reward: -200.0,e: 0.028)\n",
      "episode: 358/2000, total reward: -200.0,e: 0.027)\n",
      "episode: 359/2000, total reward: -200.0,e: 0.027)\n",
      "updated target model\n",
      "episode: 360/2000, total reward: -200.0,e: 0.027)\n",
      "episode: 361/2000, total reward: -200.0,e: 0.027)\n",
      "updated target model\n",
      "episode: 362/2000, total reward: -200.0,e: 0.026)\n",
      "episode: 363/2000, total reward: -200.0,e: 0.026)\n",
      "episode: 364/2000, total reward: -200.0,e: 0.026)\n",
      "updated target model\n",
      "episode: 365/2000, total reward: -200.0,e: 0.026)\n",
      "episode: 366/2000, total reward: -200.0,e: 0.025)\n",
      "updated target model\n",
      "episode: 367/2000, total reward: -200.0,e: 0.025)\n",
      "episode: 368/2000, total reward: -200.0,e: 0.025)\n",
      "episode: 369/2000, total reward: -200.0,e: 0.025)\n",
      "updated target model\n",
      "episode: 370/2000, total reward: -200.0,e: 0.024)\n",
      "episode: 371/2000, total reward: -200.0,e: 0.024)\n",
      "updated target model\n",
      "episode: 372/2000, total reward: -200.0,e: 0.024)\n",
      "episode: 373/2000, total reward: -200.0,e: 0.024)\n",
      "episode: 374/2000, total reward: -200.0,e: 0.023)\n",
      "updated target model\n",
      "episode: 375/2000, total reward: -200.0,e: 0.023)\n",
      "episode: 376/2000, total reward: -200.0,e: 0.023)\n",
      "updated target model\n",
      "episode: 377/2000, total reward: -200.0,e: 0.023)\n",
      "episode: 378/2000, total reward: -200.0,e: 0.022)\n",
      "episode: 379/2000, total reward: -200.0,e: 0.022)\n",
      "updated target model\n",
      "episode: 380/2000, total reward: -200.0,e: 0.022)\n",
      "episode: 381/2000, total reward: -200.0,e: 0.022)\n",
      "updated target model\n",
      "episode: 382/2000, total reward: -200.0,e: 0.022)\n",
      "episode: 383/2000, total reward: -200.0,e: 0.021)\n",
      "episode: 384/2000, total reward: -200.0,e: 0.021)\n",
      "updated target model\n",
      "episode: 385/2000, total reward: -200.0,e: 0.021)\n",
      "episode: 386/2000, total reward: -200.0,e: 0.021)\n",
      "updated target model\n",
      "episode: 387/2000, total reward: -200.0,e: 0.02)\n",
      "episode: 388/2000, total reward: -200.0,e: 0.02)\n",
      "episode: 389/2000, total reward: -200.0,e: 0.02)\n",
      "updated target model\n",
      "episode: 390/2000, total reward: -200.0,e: 0.02)\n",
      "episode: 391/2000, total reward: -200.0,e: 0.02)\n",
      "updated target model\n",
      "episode: 392/2000, total reward: -200.0,e: 0.019)\n",
      "episode: 393/2000, total reward: -200.0,e: 0.019)\n",
      "episode: 394/2000, total reward: -200.0,e: 0.019)\n",
      "updated target model\n",
      "episode: 395/2000, total reward: -200.0,e: 0.019)\n",
      "episode: 396/2000, total reward: -200.0,e: 0.019)\n",
      "updated target model\n",
      "episode: 397/2000, total reward: -200.0,e: 0.019)\n",
      "episode: 398/2000, total reward: -200.0,e: 0.018)\n",
      "episode: 399/2000, total reward: -200.0,e: 0.018)\n",
      "updated target model\n",
      "episode: 400/2000, total reward: -200.0,e: 0.018)\n",
      "episode: 401/2000, total reward: -200.0,e: 0.018)\n",
      "updated target model\n",
      "episode: 402/2000, total reward: -200.0,e: 0.018)\n",
      "episode: 403/2000, total reward: -200.0,e: 0.017)\n",
      "episode: 404/2000, total reward: -200.0,e: 0.017)\n",
      "updated target model\n",
      "episode: 405/2000, total reward: -200.0,e: 0.017)\n",
      "episode: 406/2000, total reward: -200.0,e: 0.017)\n",
      "updated target model\n",
      "episode: 407/2000, total reward: -200.0,e: 0.017)\n",
      "episode: 408/2000, total reward: -200.0,e: 0.017)\n",
      "episode: 409/2000, total reward: -200.0,e: 0.016)\n",
      "updated target model\n",
      "episode: 410/2000, total reward: -200.0,e: 0.016)\n",
      "episode: 411/2000, total reward: -200.0,e: 0.016)\n",
      "updated target model\n",
      "episode: 412/2000, total reward: -200.0,e: 0.016)\n",
      "episode: 413/2000, total reward: -200.0,e: 0.016)\n",
      "episode: 414/2000, total reward: -200.0,e: 0.016)\n",
      "updated target model\n",
      "episode: 415/2000, total reward: -200.0,e: 0.015)\n",
      "episode: 416/2000, total reward: -200.0,e: 0.015)\n",
      "updated target model\n",
      "episode: 417/2000, total reward: -200.0,e: 0.015)\n",
      "episode: 418/2000, total reward: -200.0,e: 0.015)\n",
      "episode: 419/2000, total reward: -200.0,e: 0.015)\n",
      "updated target model\n",
      "episode: 420/2000, total reward: -200.0,e: 0.015)\n",
      "episode: 421/2000, total reward: -200.0,e: 0.015)\n",
      "updated target model\n",
      "episode: 422/2000, total reward: -200.0,e: 0.014)\n",
      "episode: 423/2000, total reward: -200.0,e: 0.014)\n",
      "episode: 424/2000, total reward: -200.0,e: 0.014)\n",
      "updated target model\n",
      "episode: 425/2000, total reward: -200.0,e: 0.014)\n",
      "episode: 426/2000, total reward: -200.0,e: 0.014)\n",
      "updated target model\n",
      "episode: 427/2000, total reward: -200.0,e: 0.014)\n",
      "episode: 428/2000, total reward: -200.0,e: 0.014)\n",
      "episode: 429/2000, total reward: -200.0,e: 0.013)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated target model\n",
      "episode: 430/2000, total reward: -200.0,e: 0.013)\n",
      "episode: 431/2000, total reward: -200.0,e: 0.013)\n",
      "episode: 432/2000, total reward: -200.0,e: 0.013)\n",
      "updated target model\n",
      "episode: 433/2000, total reward: -200.0,e: 0.013)\n",
      "episode: 434/2000, total reward: -200.0,e: 0.013)\n",
      "updated target model\n",
      "episode: 435/2000, total reward: -200.0,e: 0.013)\n",
      "episode: 436/2000, total reward: -200.0,e: 0.013)\n",
      "episode: 437/2000, total reward: -200.0,e: 0.012)\n",
      "updated target model\n",
      "episode: 438/2000, total reward: -200.0,e: 0.012)\n",
      "episode: 439/2000, total reward: -200.0,e: 0.012)\n",
      "updated target model\n",
      "episode: 440/2000, total reward: -200.0,e: 0.012)\n",
      "episode: 441/2000, total reward: -200.0,e: 0.012)\n",
      "episode: 442/2000, total reward: -200.0,e: 0.012)\n",
      "updated target model\n",
      "episode: 443/2000, total reward: -200.0,e: 0.012)\n",
      "episode: 444/2000, total reward: -200.0,e: 0.012)\n",
      "updated target model\n",
      "episode: 445/2000, total reward: -200.0,e: 0.011)\n",
      "episode: 446/2000, total reward: -200.0,e: 0.011)\n",
      "episode: 447/2000, total reward: -200.0,e: 0.011)\n",
      "updated target model\n",
      "episode: 448/2000, total reward: -200.0,e: 0.011)\n",
      "episode: 449/2000, total reward: -200.0,e: 0.011)\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "done = False\n",
    "rewards = []\n",
    "losses=[]\n",
    "update_target_step = 500\n",
    "step_counter = 0\n",
    "losses = []\n",
    "steps = 0\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for time in range(600):\n",
    "        \n",
    "        if e % 50 <= 4:\n",
    "            env.render()\n",
    "\n",
    "        action = act(state)\n",
    "    \n",
    "\n",
    "        next_state, reward, done,_ = env.step(action)\n",
    "        \n",
    "        reward_i = rnd.get_reward(torch.FloatTensor(state).unsqueeze(0)).detach().clamp(-1.0,1.0).item()\n",
    "        reward_comb  =  reward + reward_i\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        store(state,action,reward_comb,next_state,done)\n",
    "        \n",
    "        if done:\n",
    "            print('episode: {}/{}, total reward: {},e: {:.2})'.format(e,n_episodes,total_reward,eps))\n",
    "            rewards.append(total_reward)\n",
    "            break\n",
    "        \n",
    "        if len(memory)>= batch_size:\n",
    "            loss = update(memory,batch_size)\n",
    "            \n",
    "       \n",
    "\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        step_counter += 1\n",
    "        if (step_counter > update_target_step):\n",
    "                target_model.load_state_dict(dqn.state_dict())\n",
    "                step_counter = 0\n",
    "                print('updated target model')\n",
    "\n",
    "    if eps > eps_min:            \n",
    "        eps = eps*eps_decay\n",
    "            \n",
    "    losses.append(loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(rewards);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for t in range(200):\n",
    "    env.render()\n",
    "    x = torch.FloatTensor(obs)\n",
    "    Q = dqn(x)\n",
    "    action = Q.argmax().detach().item()\n",
    "    new_obs,reward,done,_ = env.step(action)\n",
    "    obs = new_obs\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
