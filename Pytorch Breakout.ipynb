{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque,namedtuple\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import RAdam,Adam\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env('BreakoutNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VecFrameStack(env,n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new,r,done,_ = env.step([3])\n",
    "plt.imshow(x_new.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new,r,done,_ = env.step([3])\n",
    "plt.imshow(x_new.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VecTransposeImage(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sz = env.observation_space.shape\n",
    "action_sz = env.action_space.n\n",
    "print('State space: ',state_sz)\n",
    "print('Action space: ',action_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4)),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2)),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1)),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Flatten(start_dim=1, end_dim=-1),\n",
    "                                nn.Linear(in_features=3136, out_features=512, bias=True),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(in_features=512, out_features=4, bias=True))\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        return self(x).argmax(dim=-1).cpu().numpy()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor(x).to(torch.float).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_transitions=10000000\n",
    "batch_size=32\n",
    "gamma = 0.99\n",
    "lr = 0.0001\n",
    "eps = 1.0\n",
    "decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_sz = int(1e6)\n",
    "replay_buffer = deque(maxlen=buffer_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = namedtuple('transition',['x_new','reward','x','action','done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(transition):\n",
    "    replay_buffer.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = dqn.cuda()\n",
    "target = copy.deepcopy(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(dqn.parameters(),lr=lr)\n",
    "loss_fn = nn.HuberLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    \n",
    "    if len(replay_buffer)<batch_size:\n",
    "        return\n",
    "    \n",
    "    batch = random.sample(replay_buffer,batch_size)\n",
    "    \n",
    "    x = torch.FloatTensor(np.array([t.x for t in batch])).squeeze(1).cuda()\n",
    "    r = torch.FloatTensor(np.array([t.reward for t in batch])).cuda()\n",
    "    x_new = torch.FloatTensor(np.array([t.x_new for t in batch])).squeeze(1).cuda()\n",
    "    a = torch.LongTensor(np.array([t.action for t in batch])).unsqueeze(1).cuda()\n",
    "    done = torch.FloatTensor(np.array([t.done for t in batch])).squeeze(1).cuda()\n",
    "    \n",
    "    a_ = dqn(x_new).argmax(dim=-1).unsqueeze(1)\n",
    "    \n",
    "    target_q = (r + gamma*target(x_new).gather(1,a_).squeeze(1)*(1-done).squeeze())\n",
    "\n",
    "    \n",
    "    prediction_q = dqn(x).gather(1,a)\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = loss_fn(target_q,prediction_q.squeeze())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step() \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,target_update_frequency=10000,eps=1):\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.target_update_frequency = target_update_frequency\n",
    "        self.target_update_counter = 0\n",
    "        self.total_rewards = 0.0\n",
    "        self.total_transitions = 0\n",
    "        self.episodes = 0\n",
    "        self.episode_reward = 0.0\n",
    "        \n",
    "        \n",
    "    def select_action(self,x,eps):\n",
    "        \n",
    "        t = np.random.random()\n",
    "        if t < eps:\n",
    "            a = np.random.choice(range(action_sz))\n",
    "        else:\n",
    "            q = dqn(torch.FloatTensor(x).cuda())\n",
    "            a = q.argmax().item()   \n",
    "        return a\n",
    "            \n",
    "        \n",
    "        \n",
    "    def run_episode(self,render):\n",
    "\n",
    "        x = env.reset()\n",
    "        self.episode_reward = 0\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        transition_count = 0\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            self.target_update_counter += 1\n",
    "            \n",
    "            if self.eps > 0.1:\n",
    "                self.eps -= decay\n",
    "            else:\n",
    "                self.eps = 0.1\n",
    "\n",
    "            action = self.select_action(x,self.eps)\n",
    "            \n",
    "            x_new,reward,done,_ = env.step([action])\n",
    "\n",
    "            transition_count+=1\n",
    "                    \n",
    "            x = x_new\n",
    "            \n",
    "            t = transition(x_new,float(reward.item()),x,action,done)\n",
    "            store(t)\n",
    "            \n",
    "            update()\n",
    "            \n",
    "            self.episode_reward += reward.item()\n",
    "\n",
    "            done = done \n",
    "\n",
    "        if self.target_update_counter >= self.target_update_frequency:\n",
    "\n",
    "                self.target_update_counter = 0\n",
    "                target.load_state_dict(dqn.state_dict())\n",
    "                print('Target network updated')\n",
    "                \n",
    "        self.total_rewards += self.episode_reward\n",
    "        self.total_transitions += transition_count\n",
    "            \n",
    "\n",
    "#             print('Running Average',np.mean(self.rewards[-20:]))\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        while self.total_transitions <= n_transitions:\n",
    "            \n",
    "            for i in range(4):\n",
    "                self.run_episode(False)\n",
    "    \n",
    "            print('Total Transitions',self.total_transitions)\n",
    "            print('Avg. Reward per Episode',self.total_rewards/4)\n",
    "    \n",
    "            print('\\n --------------------------------------------------------------')\n",
    "        \n",
    "            self.total_rewards = 0.0\n",
    "\n",
    "            \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "torch.tensor(obs).cuda().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn(torch.FloatTensor(obs).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(10000):\n",
    "    action = dqn.predict(torch.FloatTensor(obs).cuda())\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
